{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-8QoaJmXd8Z"
   },
   "source": [
    "# DL Indaba Practical 2\n",
    "# Feedforward Neural Networks on Real Data & Best Practices\n",
    "*Developed by Stephan Gouws, Avishkar Bhoopchand & Ulrich Paquet.*\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this practical we will move on and discuss best practices for building and training models on real world data (the famous MNIST dataset of hand-written images of digits). We will develop a deep, fully-connected (\"feed-forward\") neural network model that can classify these images with around 98% accuracy (close to state-of-the-art for feedforward models on this dataset). \n",
    "\n",
    "**Learning objectives**:\n",
    "\n",
    "Understanding the issues involved in implementing and applying deep neural networks in practice (w/ a focus on TensorFlow). In particular:\n",
    "\n",
    "* Implementation sanity checking details & controlled overfitting on a small training set.\n",
    " \n",
    "* Training deep neural networks. Understand:\n",
    " * Conceptually how the backprop algorithm efficiently computes model gradients by applying the chain rule and applying dynamic programming (saving intermediate computations).\n",
    " * Overfitting and how to use regularization to avoid it (Weight decay/L2 and dropout).\n",
    " * Knowing when to stop training (early stopping).\n",
    " \n",
    "* Understand the need for hyperparameter tuning in finding good architectures and training settings. \n",
    "\n",
    "**What is expected of you:**\n",
    "\n",
    "* We have included rough time estimates of how long you should aim to spend on the important sections below.\n",
    "* Step through the notebook answering the questions by discussing them with your lab partner. \n",
    "* Execute each cell in turn & fill in the missing code by pair-programming with your lab partner. \n",
    "* 5 min before the end, pair up with someone else and make sure you both understand the concepts listed in \"Learning Objectives\" above. If not, please speak to the tutors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JQ4eAF7X2tK"
   },
   "source": [
    "# Setups and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2tOik9MR8_O"
   },
   "source": [
    "For this practical, we will work with the [famous MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/). The task is to classify which digit a particular image represents. Luckily, since MNIST is a very popular dataset, TensorFlow has some built-in functions to download it. \n",
    "\n",
    "The MNIST dataset consists of pairs of images (28x28 matrices) and labels. Each label\n",
    "is represented as a (sparse) binary    vector of length 10 with a 1 in position    `i` iff the image represents digit `i`, and 0 elsewhere. This is what the \"one_hot\" parameter you see below does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 153,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8487,
     "status": "ok",
     "timestamp": 1503657242635,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "IYrNES0hD6Mi",
    "outputId": "258b73ac-670c-4b30-9b67-7fbe07be7c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-3-73fcaabaeedf>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/luyolomagangane/Virtual-Environments/venv_deep_learning_p3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and some other libraries we'll be using.\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Download the MNIST dataset onto the local machine.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdz38ZyZE3vW"
   },
   "source": [
    "# Visualizing the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmblsKmbl9sY"
   },
   "source": [
    "Let's visualize a few of the digits (from the training set): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 208,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 999,
     "status": "ok",
     "timestamp": 1503055879874,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "JpAbMODcG1se",
    "outputId": "76aa84c7-ee87-45ba-e73d-1476f54ec647"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.ioff()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Helper plotting routine.\n",
    "def display_images(gens, title=\"\"):\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(25, 3))\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i in range(10):\n",
    "        reshaped_img = (gens[i].reshape(28, 28) * 255).astype(np.uint8)\n",
    "        axs.flat[i].imshow(reshaped_img)\n",
    "        # axs.flat[i].axis('off')        \n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "batch_xs, batch_ys = mnist.train.next_batch(10)\n",
    "list_of_images = np.split(batch_xs, 10)\n",
    "_ = display_images(list_of_images, \"Some Examples from the Training Set.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrVjQKHlT2m4"
   },
   "source": [
    "# Building a Feed-Forward Neural Network\n",
    "\n",
    "In this section, we will build a neural network that takes the raw MNIST pixels as inputs and outputs 10 values, which we will interpret as the probability that the input image belongs to one of the 10 digit classes (0 through 9). Along the way, the data will pass through the hidden layers and activation functions we encountered in the first practical. \n",
    "\n",
    "**NOTE**: Standard feedforward neural network architectures can be summarised by chaining together the number of neurons in each layer, e.g. \"784-500-300-10\" would be a net with 784 input neurons, followed by a layer with 500 neurons, then 300, and finally 10 output classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2L7D76zYZZv"
   },
   "source": [
    "## Build the model (30min)\n",
    "\n",
    "We want to explore the different choices and some of the best practices of training feedforward neural networks on the MNIST dataset. In this practical we will only be using \"fully connected\" (also referred to as \"FC\", \"affine\", or \"dense\") layers (i.e. no convolutional layers). So let's first write a little helper function to construct these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "my3WeXTdEt34"
   },
   "outputs": [],
   "source": [
    "def _dense_linear_layer(inputs, layer_name, input_size, output_size):\n",
    "    \"\"\"\n",
    "    Builds a layer that takes a batch of inputs of size `input_size` and returns \n",
    "    a batch of outputs of size `output_size`.\n",
    "         \n",
    "    Args:\n",
    "        inputs: A `Tensor` of shape [batch_size, input_size].\n",
    "        layer_name: A string representing the name of the layer.\n",
    "        input_size: The size of the inputs\n",
    "        output_size: The size of the outputs\n",
    "        \n",
    "    Returns:\n",
    "        out, weights: tuple of layer outputs and weights.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Name scopes allow us to logically group together related variables.\n",
    "    # Setting reuse=False avoids accidental reuse of variables between different runs.\n",
    "    with tf.variable_scope(layer_name, reuse=False):\n",
    "        # Create the weights for the layer\n",
    "        layer_weights = tf.get_variable(\"weights\",\n",
    "                                        shape=[input_size, output_size], \n",
    "                                        dtype=tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer())\n",
    "        # Create the biases for the layer\n",
    "        layer_bias = tf.get_variable(\"biases\", \n",
    "                                     shape=[output_size], \n",
    "                                     dtype=tf.float32, \n",
    "                                     initializer=tf.random_normal_initializer())\n",
    "        \n",
    "        ## IMPLEMENT-ME: (1) \n",
    "                            #.      [1 * 784] [784 x 10]\n",
    "        outputs = tf.add(tf.matmul(inputs, layer_weights), layer_bias)\n",
    "    \n",
    "    return outputs, layer_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTFPjWj7FqJy"
   },
   "source": [
    "Now let's use this to construct a linear softmax classifier as before, which we will expand into a near state-of-the-art feed-forward model for MNIST. We first create an abstract `BaseSoftmaxClassifier` base class that houses common functionality between the models. Each specific model will then provide a `build_model` method that represents the logic of that specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oaHvemlxVeIi"
   },
   "outputs": [],
   "source": [
    "class BaseSoftmaxClassifier:\n",
    "    def __init__(self, input_size, output_size, l2_lambda):        \n",
    "        # Define the input placeholders. The \"None\" dimension means that the \n",
    "        # placeholder can take any number of images as the batch size. \n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size])    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        self._all_weights = [] # Used to compute L2 regularization in compute_loss().\n",
    "        \n",
    "        # You should override these in your build_model() function.\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        self.loss = None\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def get_logits(self):\n",
    "        return self.logits\n",
    "    \n",
    "    def build_model(self):\n",
    "        # OVERRIDE THIS FOR YOUR PARTICULAR MODEL.\n",
    "        raise NotImplementedError(\"Subclasses should implement this function!\")\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        \"\"\"All models share the same softmax cross-entropy loss.\"\"\"\n",
    "        assert self.logits is not None    # Ensure that logits has been created! \n",
    "        \n",
    "        # IMPLEMENT-ME: (2)\n",
    "        # HINT: This time, use the TensorFlow function tf.nn.softmax_cross_entropy_with_logits  rather than \n",
    "        # implementing it manually like we did in Prac1\n",
    "        data_loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "\n",
    "        reg_loss = 0.\n",
    "        for w in self._all_weights:\n",
    "            # IMPLEMENT-ME: (3)\n",
    "            # HINT: TensorFlow has a built-in function for this too! tf.nn.l2_loss\n",
    "            reg_loss += tf.nn.l2_loss(w)\n",
    "            \n",
    "        return data_loss + self.l2_lambda * reg_loss\n",
    "    \n",
    "    def accuracy(self):\n",
    "        # Calculate accuracy.\n",
    "        assert self.predictions is not None    # Ensure that pred has been created!\n",
    "        \n",
    "        # IMPLEMENT-ME: (4)\n",
    "        # HINT: Look up the tf.equal and tf.argmax functions\n",
    "        correct_prediction = tf.equal(tf.argmax(self.predictions, 1), tf.argmax(self.y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVqOqr8qKApu"
   },
   "source": [
    "If we wanted to reimplement the linear softmax classifier from before, we just need to override `build_model()` to perform one projection from the input to the output logits, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jLVw-NWEKBI9"
   },
   "outputs": [],
   "source": [
    "class LinearSoftmaxClassifier(BaseSoftmaxClassifier):\n",
    "    def __init__(self, intput_size, output_size, l2_lambda):\n",
    "        super().__init__(input_size, output_size, l2_lambda)\n",
    "        \n",
    "    def build_model(self):\n",
    "        # The model takes x as input and produces output_size outputs.\n",
    "        self.logits, weights = _dense_linear_layer(\n",
    "                self.x, \"linear_layer\", self.input_size, self.output_size)\n",
    "        \n",
    "        self._all_weights.append(weights)\n",
    "        \n",
    "        self.predictions = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        self.loss = self.compute_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Lxtrw0KnmW"
   },
   "source": [
    "In order to build a ***deeper*** model, let's add several layers with multiple transformations and rectied linear units (relus):\n",
    "\n",
    "```python\n",
    "def build_model(self):\n",
    "     # The first layer takes x as input and has n_hidden_1 outputs.\n",
    "     layer1, weights1 = _build_linear_layer(self.x, \"layer1\", self.input_size, self.num_hidden_1)\n",
    "     self._all_weights.append(weights1)\n",
    "     layer1 = tf.nn.relu(layer1)\n",
    "        \n",
    "     # The second layer takes layer1's output as its input and has num_hidden_2 outputs.\n",
    "     layer2, weights2 = _build_linear_layer(layer1, \"layer2\", self.num_hidden_1, self.num_hidden_2)\n",
    "     self._all_weights.append(weights2)\n",
    "     layer2 = tf.nn.relu(layer2)\n",
    "     \n",
    "     # The final layer is our predictions and goes from num_hidden_2 inputs to \n",
    "     # num_classes outputs. The outputs are \"logits\" (un-normalised scores). \n",
    "     self.logits, weights3 = _build_linear_layer(layer2, \"output\", self.num_hidden_2, self.output_size)\n",
    "     self._all_weights.append(weights3)\n",
    "     \n",
    "     self.pred = tf.nn.softmax(self.logits)\n",
    "     self.loss = self.compute_loss(self.logits, self.y)\n",
    " ```\n",
    "\n",
    "**Note: Instead of writing special classes for linear nets (0 hidden layers), 1 hidden layer nets, 2 hidden layer nets, etc., we can generalize this as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QVCIXYUrHXai"
   },
   "outputs": [],
   "source": [
    "class DNNClassifier(BaseSoftmaxClassifier):\n",
    "    \"\"\"DNN = Deep Neural Network - now we're doing Deep Learning! :)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size=784,    # There are 28x28 = 784 pixels in MNIST images\n",
    "                 hidden_sizes=[],    # List of hidden layer dimensions, empty for linear model.\n",
    "                 output_size=10,    # There are 10 possible digit classes\n",
    "                 act_fn=tf.nn.relu,    # The activation function to use in the hidden layers\n",
    "                 l2_lambda=0.):    # The strength of regularisation, off by default.\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.act_fn = act_fn\n",
    "        super().__init__(input_size, output_size, l2_lambda)\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        prev_layer = self.x\n",
    "        prev_size = self.input_size\n",
    "        \n",
    "        for layer_num, size in enumerate(self.hidden_sizes):        \n",
    "            layer_name = \"layer_\" + str(layer_num)\n",
    "            ## IMPLEMENT-ME: (5)\n",
    "            # HINT: Use the linear function we defined earlier!\n",
    "            layer, weights = _dense_linear_layer(prev_layer, layer_name, prev_size, size)\n",
    "\n",
    "            self._all_weights.append(weights)\n",
    "            ## IMPLEMENT-ME: (6)\n",
    "            # HINT: What do we still need to do after doing the \"linear\" part? \n",
    "            layer = self.act_fn(layer)\n",
    "            prev_layer, prev_size = layer, size\n",
    "\n",
    "        # The final layer is our predictions and goes from prev_size inputs to \n",
    "        # output_size outputs. The outputs are \"logits\", un-normalised scores. \n",
    "        self.logits, out_weights = _dense_linear_layer(prev_layer, \"output\", prev_size, self.output_size)\n",
    "        self._all_weights.append(out_weights)\n",
    "        \n",
    "        self.predictions = tf.nn.softmax(self.logits)\n",
    "        self.loss = self.compute_loss()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHkH_jEc9C3j"
   },
   "source": [
    "We can now create a linear model, i.e. a 784-10 architecture (note that there is only one possible linear model going from 784 inputs to 10 outputs), as follows:\n",
    "\n",
    "```python\n",
    "    tf_linear_model = DNNClassifier(input_size=784, hidden_sizes=[], output_size=10)\n",
    "```\n",
    "\n",
    "We can create a deep neural network (DNN, also called a \"multi-layer perceptron\") model, e.g. a 784-512-10 architecture (there can be many others...), as follows:\n",
    "\n",
    "```python\n",
    "    tf_784_512_10_model = DNNClassifier(input_size=784, hidden_sizes=[512], output_size=10)\n",
    "```\n",
    "\n",
    "and so forth.\n",
    "\n",
    "**NOTE: Make sure you understand how this works before you move on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVynk3SWzUg_"
   },
   "source": [
    "## Sanity Checks (5-10 min)\n",
    "\n",
    "### Dealing with randomness\n",
    "\n",
    "Pseudo-random number generators start from some value (the 'seed') and generate numbers which appear to be random. It is good practise to seed RNGs with a fixed value to encourage reproducibility of results. We do this in NumPy by using `np.random.seed(1234)`, where `1234` is your chosen seed value. \n",
    "\n",
    "In TensorFlow we can set a *graph-level* seed (global, using `tf.set_random_seed(1234)`) or an *op-level* seed (passed in to each op via the `seed` argument, to override the graph-level seed.)\n",
    "\n",
    "Next we need to initialize the parameters of our model. We do *not* want to initialize all weights to be the same. \n",
    "\n",
    "**QUESTION**: Can you think of why this might be a bad idea? (Think about a simple MLP with one input and 2 hidden units and one output. Look at the contributions via each weight connection to the activations on the hidden layer. Now let those weights be equal. How does that affect the contributions? How does that affect the update that backprop would propose for each weight? Do you see any problems?)\n",
    "\n",
    "The simplest approach is to **initialize weights (W's) to small random values**. For ReLUs specifically, it is recommended to initialize weights to `np.random.randn(n) * sqrt(2.0/n)`, where `n` is the number of inputs to that layer (neuron on the previous layer) (see [He et al., 2015](https://arxiv.org/pdf/1502.01852.pdf) for more information). This initialization encourages the distributions over ReLU activations to have roughly the same variance at each layer, which in turn ensures that useful gradient information gets sent back during backpropagation.\n",
    "\n",
    "Biases are not that sensitive to initialization and can be initialized to 0s or small random numbers.\n",
    "\n",
    "### Check the loss of a random model\n",
    "\n",
    "When we have a new model, it's always a good idea to do a quick sanity check. A random model that predicts C classes on random data should have no reason for preferring either class, i.e., on average its loss (negative log-likelihood) should be $-\\log(1/C) = -\\log(C^{-1}) = \\log(C)$. \n",
    "\n",
    "**NOTE**: TF actually computes the cross-entropy on the *logits* for numerical stability reasons (logs of small numbers blow up quickly..). This means we'll get a different value from `tf.nn.softmax_cross_entropy_with_logits`. So for this check we will just manually compute the cross-entropy (negative log-likelihood).\n",
    "\n",
    "The second thing to check is that adding the L2 loss (a strictly positive value), should *increase* total loss (here we'll just use the TF cross-entropy as provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 408,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1503657319275,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "DUMQp9kw0Isx",
    "outputId": "b8d7a421-e489-4b54-a896-aa2da2435020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of random data:\n",
      " [[ 0.47143516 -1.19097569  1.43270697 ... -0.2453605  -1.26943186\n",
      "  -0.26232386]\n",
      " [ 2.33759848 -0.78171744  0.08009975 ... -0.64055353  1.76256841\n",
      "  -0.08567302]\n",
      " [ 1.63617833 -0.54410827 -1.04999868 ... -0.90640906  0.31915076\n",
      "  -0.49914386]\n",
      " [-0.66039926  0.0774697   0.38755182 ...  0.31019053  1.87791254\n",
      "  -0.70183467]\n",
      " [-3.23350453  0.20024296 -0.13933709 ... -0.72622006  0.50774695\n",
      "   0.18987136]]\n",
      "Shape:  (100, 784)\n",
      "Sample of random labels:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "Shape:  (100, 10)\n",
      "WARNING:tensorflow:From <ipython-input-6-6550f5ac3e9e>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "\n",
      "Sanity check manual avg cross entropy:  2.3025851\n",
      "Model loss (no reg):  46.757496\n",
      "Sanity check loss (with regularization, should be higher):  3836.2979\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Generate a batch of 100 \"images\" of 784 pixels consisting of Gaussian noise.\n",
    "x_rnd = np.random.randn(100, 784)\n",
    "print(\"Sample of random data:\\n\", x_rnd[:5,:])    # Print the first 5 \"images\"\n",
    "print(\"Shape: \", x_rnd.shape)\n",
    "# Generate some random one-hot labels.\n",
    "y_rnd = np.eye(10)[np.random.choice(10, 100)]\n",
    "print(\"Sample of random labels:\\n\", y_rnd[:5,:])\n",
    "print(\"Shape: \", y_rnd.shape)\n",
    "\n",
    "# Model without regularization.\n",
    "tf.reset_default_graph()\n",
    "tf_linear_model = DNNClassifier(l2_lambda=0.0)\n",
    "x, y = tf_linear_model.x, tf_linear_model.y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    avg_cross_entropy = -tf.log(tf.reduce_mean(tf_linear_model.predictions))\n",
    "    loss_no_reg = tf_linear_model.loss\n",
    " \n",
    "    manual_avg_xent, loss_no_reg = sess.run([avg_cross_entropy, loss_no_reg],\n",
    "                                            feed_dict={x : x_rnd, y: y_rnd})\n",
    "    \n",
    "    \n",
    "# Sanity check: Loss should be about log(10) = 2.3026\n",
    "print('\\nSanity check manual avg cross entropy: ', manual_avg_xent)\n",
    "print('Model loss (no reg): ', loss_no_reg)\n",
    "\n",
    "# Model with regularization.\n",
    "tf.reset_default_graph()\n",
    "tf_linear_model = DNNClassifier(l2_lambda=1.0)\n",
    "x, y = tf_linear_model.x, tf_linear_model.y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    loss_w_reg = tf_linear_model.loss.eval(feed_dict={x : x_rnd, y: y_rnd})\n",
    "\n",
    "# Sanity check: Loss should go up when you add regularization\n",
    "print('Sanity check loss (with regularization, should be higher): ', loss_w_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0YXZ-u284tK"
   },
   "source": [
    "## Compute the gradients: Backpropagation Review\n",
    "\n",
    "Once one has implemented the model and checked that the loss produces sensible outputs it is time to create the training loop. For this, we will need to obtain the gradients of the loss wrt each model parameter. We've already looked at this in detail in practical 1, where we performed this manually. One of the great benefits of using a library like TensorFlow, is that it can automatically derive the gradients wrt any parameter in the graph (using `tf.gradients()`).\n",
    "\n",
    "In the previous practical we saw that there is a common pattern to deriving the gradients in neural networks:\n",
    "\n",
    "1. propagate activations forward through the network (\"make a prediction\" $\\Rightarrow$ `fprop`),\n",
    "2. compute an error delta (\"see how far we're off\") , and\n",
    "3. propagate errors backwards to update the weights (\"update the weights to do better next time\" $\\Rightarrow$ `backprop`).\n",
    "\n",
    "It turns out that for deeper networks, ***this pattern repeats for every new layer***. Take a deep breath, and let's dive in, starting with fprop:\n",
    "\n",
    "### FPROP\n",
    "\n",
    "Conceptually, forward propagation is very simple: Starting with the input `x`, we repeatedly apply an affine function and a non-linearity to arrive at the output $a^L = \\sigma^L(W^{L-1}\\sigma^{L-1}( \\ldots \\sigma(W^1x + b^1) \\dots ) + b^{L-1})$. \n",
    "\n",
    "Mathematically, forward propagation comes down to a **composition of functions**. So for two layers, the output activation $a^2$ is the composition of the function $a^1 = f^1(x)$ and $a^2 = f^2(a^1)$ $\\Rightarrow$ $a^2 = f^2(f^1(x))$.\n",
    "\n",
    "**NOTE**: We save both the pre-activations (before the non-linearity) *and* the (post-)activations (after the nonlinearity). We'll need these for the backprop phase!\n",
    "\n",
    "In code it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BKxO41QtW0ka"
   },
   "outputs": [],
   "source": [
    "# PSEUDOCODE:\n",
    "\n",
    "def fprop(x, weights, biases, per_layer_nonlinearities):\n",
    "\n",
    "    # Initialise the input. We pretend inputs are the first pre- and post-activations.\n",
    "    z = a = x\n",
    "    cache = [(z, a)] # We'll save z's and a's for the backprop phase.\n",
    "    \n",
    "    for W, b, act_fn in zip(weights, biases, per_layer_nonlinearities): \n",
    "        \n",
    "        z = np.dot(W, a) + b        # \"pre-activations\" / logits\n",
    "        a = act_fn(z)               # \"outputs\" / (post-)activations of the current layer\n",
    "        \n",
    "        # NOTE: We save both pre-activations and (post-)activations for the backwards phase!\n",
    "        cache.append((z, a))\n",
    "            \n",
    "    return cache # Per-layer pre- and post-activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeQ6_uE5asVK"
   },
   "source": [
    "### BACKPROP\n",
    "\n",
    "Given a loss or error function $E$ at the output (e.g. cross-entropy), we then need the derivative of the loss wrt each of the model parameters in order to train the network (decrease the loss). Mathematically, this comes down to the derivative of a composition of functions, and from calculus we know we have the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) for that: If $a = f(g(x))$, then $\\frac{\\partial a}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x}$.\n",
    "\n",
    "In order to get the gradients on some intermediate parameter $\\theta^i = \\{W^i, b^i\\}$ in layer $i$, we just apply the chain rule over all $L$ 'layers' of this composition of functions (the neural network) to derive the intermediate gradients:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial E}{\\partial \\theta^{(i)}} \n",
    "        &= \\underbrace{ \\frac{\\partial E}{\\partial z^{(L)}} \\frac{\\partial z^{(L)}}{\\partial z^{(L-1)}} \\ldots \\frac{\\partial z^{(i+2)}}{\\partial z^{(i+1)}}}_{\\triangleq \\delta^{(i+1)}} \\frac{\\partial z^{(i+1)}}{\\partial \\theta^{(i)}} \\\\\n",
    "    &= \\delta^{(i+1)} \\frac{\\partial z^{(i+1)}}{\\partial \\theta^{(i)}}\n",
    "\\end{aligned}\n",
    "\n",
    "We have glossed a little over the fact that we are dealing with matrices and vectors, for the sake of brevity. But please see these two great resources:\n",
    "* For a step-by-step walk-through of the mechanics of backpropagation, see [this great resource](http://cs224d.stanford.edu/lecture_notes/notes3.pdf).\n",
    "* For a great brush-up on the mechanics of vector calculus, see [this fantastic note](https://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-gradient-notes.pdf) (for the same course).\n",
    "\n",
    "**NOTE**: \n",
    "* $E$ is the error/loss function at the output.\n",
    "* $W^{(i)}$ is an intermediate weight matrix mapping activations from layer $i$ to pre-activations $z^{(i+1)}$ at layer $i+1$.\n",
    "* The derivative with respect to the *inputs* of layer $i$ are called deltas and is defined as $\\delta^{(i)} \\triangleq \\frac{\\partial E}{\\partial z^{(i)}}$. \n",
    "    * $\\delta^i$ is a vector (the size of the layer $i$), and is the result of the deltas at the output $\\delta^L$ (dE/dlogits) multiplied by a product of [Jacobian matrices](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant). If you don't understand this statement, just think of $\\delta^i_j$ as the contribution that unit $j$ in layer $i$ made to the total error $E$. Also, read up on this in the resources linked above.\n",
    "    * **NB**: $z^{(i)}$ are the pre-activations (\"logits\" at the output layer)!\n",
    "* $\\frac{\\partial z^{(i+1)}}{\\partial W^{(i)}} = \\frac{\\partial (W^{(i)}a^i + b^i)}{\\partial W^{(i)}} = a^i$. \n",
    "* Likewise, $\\frac{\\partial z^{(i+1)}}{\\partial b^{(i)}} = 1$ (check this for yourself).\n",
    "\n",
    "\n",
    "**QUESTIONS**: \n",
    "1. What is the shape of $\\frac{\\partial E}{\\partial \\mathbb{W}^{(i)}}$?\n",
    "    * Convince yourself (now or later) that $\\frac{\\partial E}{\\partial W^{(i)}_{jk}} = \\delta^{(i+1)}_k {a^i_j}$ (a scalar), and therefore $\\frac{\\partial E}{\\partial \\mathbb{W}^{(i)}} = \\mathbb{\\delta}^{(i+1)} {\\mathbb{a}^i}^T$ (outer product of two vectors, therefore a matrix of the same shape as $W^i$). \n",
    "    * **In words: The gradient on weights $W^i$ at layer $i$ is the outer product of the deltas-vector from the layer above $\\delta^{(i+1)}$ and the activations vector from the layer below $a^i$.**\n",
    "\n",
    "2. What is the shape of $\\frac{\\partial E}{\\partial \\mathbb{b}^{(i)}}$?\n",
    "    * Convince yourself (now or later) that $\\frac{\\partial E}{\\partial \\mathbb{b}^{(i)}} = \\mathbb{\\delta}^{(i+1)}$ (a vector of the same length as $b^i$).\n",
    "    * **In words: The gradient on biases $b^i$ at layer $i$ is equal to the deltas at the layer above**.\n",
    "3. Make sure you understand why we save the `(z,a)`'s during fprop, and how we use them during backprop.\n",
    "\n",
    "\n",
    "Note that we need to compute the deltas at every layer, and $\\delta^i$ share all the terms of $\\delta^{(i+1)}$ (which we've already computed), except one. Backprop has one more trick up its sleeve, and that is to *reuse previously computed values to save computation* (this is called [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)). For the deltas, this comes down to computing $\\delta^{(i)}$ given $\\delta^{(i+1)}$. We won't show the full derivation (but we again use the chain rule, see the notes linked above) to arrive at:\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta^{(i)} &= \\frac{\\partial E}{\\partial z^{(i)}} \\\\\n",
    "                             &= \\ldots \\\\\n",
    "                             &= \\underbrace{ \\left[ \\delta^{(i+1)} {\\mathbb{W}^{(i)}}^T \\right] }_\\textrm{Map the global delta 'backwards' through W up to layer $i$} \\circ \\underbrace{ \\sigma'(z^{(i)}) }_\\textrm{Correct for the local errors at this layer.}.\n",
    "\\end{align}\n",
    "\n",
    "**In words: The delta on the current layer $i$ is the delta on the layer above $\\delta^{(i+1)}$ multiplied by the transpose weights matrix between the two layers $W^i$ to yield a vector, scaled by the element-wise multiplication of $\\sigma'(z^{(i)})$ (the derivative of the non-linearity applied to the original pre-activations).**\n",
    "\n",
    "**QUESTIONs**: Look at the code below and make sure you understand (at least conceptually) how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_tNoN3q4azpx"
   },
   "outputs": [],
   "source": [
    "# PSEUDOCODE\n",
    "\n",
    "def backprop(target, fprop_cache, weights):\n",
    "    \n",
    "    # Pop/remove the model prediction (last activation `a` we computed above) \n",
    "    # off the cache we created during the fprop phase.\n",
    "    (_, pred) = fprop_cache.pop() \n",
    "    \n",
    "    # Intialise delta^{L} (at the output layer) as dE/dz (cross-entropy).\n",
    "    delta_above = (target - pred)         \n",
    "    grads = []\n",
    "    \n",
    "    # Unroll backwards from the output:\n",
    "    for (z_below, a_below), W_between in reversed(zip(fprop_cache, weights)):\n",
    "        \n",
    "        # Compute dE/dW:\n",
    "        Wgrad = np.dot(delta_above, a_below.T) # Outer product\n",
    "        \n",
    "        # Compute dE/db:\n",
    "        bgrad = delta_above     \n",
    "        \n",
    "        # Save these:\n",
    "        grads.append((Wgrad, bgrad))\n",
    "        \n",
    "        # Update for the *next* iteration/layer. Note the elem-wise multiplication.\n",
    "        # Note the use of z_below, the preactivations in the layer below!\n",
    "        # delta^i = delta^{(i+1)}.(W^i)^T .* sigma'(z_i):\n",
    "        delta_above = np.dot(delta_above, W_between.T) * dsigmoid(z_below)\n",
    "    \n",
    "    grads.reverse()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvvygszheFJ1"
   },
   "source": [
    "Ppphhhhhheeeeewwwww, ok, come up for a breather. We know. This takes a while to wrap your head around. For now, just try to get the high-level picture! Depending on your background, this may well be the toughest part of the week. \n",
    "\n",
    "**BACKPROP SUMMARY NOTE**: It helps to have a good idea of what backprop does, and for this it helps to work through one example by hand (see the linked notes above). But you don't need to understand every detail above for the rest of this and the following lectures. The good news is that in practise you won't compute gradients by hand, and all of the above is done for us by `tf.gradients()` (or the similar function in other packages)!\n",
    "\n",
    "**BACKPROP FINAL QUESTION**:    Take a few minutes to explain the high-level details of the backprop algorithm to your neighbour. Try to understand how fprop is essentially just a composition of functions applied to the input, and backprop 'peels off' those compositions one by one by following the chain rule. In the process it avoids recomputation by saving activations during fprop, and computing deltas during backprop. Be sure to ask the tutors if you're stuck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "McPVC7ejezyZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# PSEUDOCODE:\\n\\nbiases, weights = [b1, b2], [W1, W2]\\nx, y = ..., ...\\nnon_linearities = [relu, softmax]\\n\\nfprop_cache = fprop(x, weights, biases, non_linearities)\\ngrads = backprop(y, fprop_cache, weights)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# PSEUDOCODE:\n",
    "\n",
    "biases, weights = [b1, b2], [W1, W2]\n",
    "x, y = ..., ...\n",
    "non_linearities = [relu, softmax]\n",
    "\n",
    "fprop_cache = fprop(x, weights, biases, non_linearities)\n",
    "grads = backprop(y, fprop_cache, weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCu7TiQ7Lygf"
   },
   "source": [
    "## Training deep neural networks (10min)\n",
    "\n",
    "Now that we have a model with a loss and gradients, let's write a function to train it! This will be largely similar to the `train_tf_model()` function (same name, see below) from the previous practical. However, we are introducing several new concepts in this practical:\n",
    "\n",
    "* how to update parameters using different optimizers, \n",
    "* model complexity and how to match this to your data: \n",
    "    * recognizing overfitting \n",
    "    * adding regularization (L2, dropout)\n",
    "* knowing when to stop training: early stopping,\n",
    "\n",
    "As we go through these concepts, we will show how to use them in our training function.\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Training neural networks involves solving an optimization problem. **Stochastic gradient-based methods** are by far the most popular family of techniques that are being used for this. These methods evaluate the gradient of the loss on a small part of the data (called a **mini-batch**), and then propose a small change to the weights based on the current sample (and some maintain running averages over the previous steps), that reduces the loss, before moving on to another sample of the data:\n",
    "\n",
    "```\n",
    "step = optimizer(grad(cost), learning_rate, ...) \n",
    "new_weights = old_weights + step\n",
    "```\n",
    "\n",
    "The oldest algorithm is stochastic gradient descent, but there are many others (Adagrad, AdaDelta, RMSProp, ADAM, etc.). As a general rule of thumb, ADAM or SGD with Momentum tend to work quite well out of the box, but this depends on your model and your data! See these two great blog posts for more on this: \n",
    "\n",
    "* http://ruder.io/optimizing-gradient-descent/\n",
    "* https://medium.com/towards-data-science/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n",
    "\n",
    "In our code, we can select different optimization functions by passing in a different optimizer to `train_tf_model` (see the full list here: https://www.tensorflow.org/api_guides/python/train#Optimizers) as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "optimizer = tf.train.RMSProp(...)\n",
    "results_tuple = train_tf_model(optimizer_fn=optimizer, ...)\n",
    "```\n",
    "\n",
    "### Training / validation / test splits\n",
    "\n",
    "When training supervised machine learning models, the goal is to build a model that will perform well on some test task with data that we'll obtain some time in the future. Unfortunately, until we solve time-travel, we don't yet have access to that data. So how do we train a model on data we have now, to perform well on some (unseen) data from the future? This, in a nutshell, is the statistical learning problem: we want to train a data on available data to **generalize** to unseen test data.\n",
    "\n",
    "The way we approach this is to take a dataset that we do have, and split it into a **training**, **validation**, and **test set** (split). Typically we will use ratios of 80/10/10 for example. We then train our model on the training set only, and use the validation set to make all kinds of decisions about architectural selection, hyperparameters, etc. When we're done, we evaluate our model and report its accuracy on the **test set only**. \n",
    "\n",
    "**NOTE**: We (typically) do not train on the validation set, and we **never** train on the test set. Think about why training on the test set might be a bad thing?\n",
    "\n",
    "### Model complexity, overfitting & regularization\n",
    "\n",
    "**Overfitting** occurs when improving the model's training loss (its performance on training data) comes at the expense of its **generalisation ability** (its performance on unseen test data). Generally it is a symptom of the **model complexity** increasing to fit the peculiarities (outliers) of the training data too accurately, causing it not to generalize well to new unseen (test) data. Overfitting is usually indicated when:\n",
    "\n",
    "* training & validation loss starts decreasing at different rates,\n",
    "* validation error starts increasing while training error still goes down,\n",
    "* training error reaches 0.\n",
    "\n",
    "**Underfitting** is the opposite: when a model cannot fit the training data well enough (usually a sign to train for longer or add more parameters to the model).\n",
    "\n",
    "You can think of complexity as how \"wiggly\" or \"wrinkly\" the decision boundary that the model can represent is. We can increase model complexity by adding more layers (i.e. more parameters). We can control or reduce the model complexity of an architecture using a family of techniques called **regularisers**. We've already encountered L2-regularisation (also called **weight-decay**), where we penalise the model for having very large weights. Another option is L1 regularization (which encourages sparsity of weights). Another very popular current technique is called **dropout** (we'll look at this in more detail in Practical 3). There are many others, but these two are the most popular.\n",
    "\n",
    "We will only be using L2 regularization in this practical. It can be set by passing a non-zero value to `l2_lambda` when constructing a `DNNClassifier` instance.\n",
    "\n",
    "### Early stopping\n",
    "\n",
    "Neural networks are nonlinear models and can have very complicated optimization landscapes. Stochastic gradient based methods for optimizing these loss functions do not proceed monotonically (i.e. does not just keep going up). Sometimes the loss can go down for a while before it goes up to reach a better part of parameter space later. How do we know when to stop training?\n",
    "\n",
    "**Early stopping** is one technique that helps with this. It is added to the training routine and means that we periodically evaluate the model's performance on the validation set (***Crucially! not the test set. Why?***). If the performance on the validation set starts becoming worse we know we have reached the point of overfitting (usually), so it usually makes sense to stop training and not waste any more computations. The `train_tf_model` function we build has an early-stopping feature that you can enable by passing the `stop_early=True` parameter. Have a look at the code to see how this is done. \n",
    "\n",
    "![early stopping](images/early_stopping.png \"Diagram illustrating early stopping using validation results\")\n",
    "\n",
    "For this practical, we just implemented the most basic idea of early stopping: stop training as soon as the model starts doing worse on validation data. However, there are different ways of implementing this idea. Two of the most popular are \n",
    "\n",
    "* \"early stopping with patience\": don't stop training immediately once validation accuracy degrades, but wait for P more epochs, and reset P if the model starts improving again within this timeframe,\n",
    "* training for T epochs, and simply selecting the best model based on validation score over the entire T epochs.\n",
    "\n",
    "**QUESTION**: What are the pros and cons of these different methods?\n",
    "\n",
    "\n",
    "\n",
    "## Wrapping these ideas into the training function (10-15min)\n",
    "\n",
    "The training function below implements all the ideas we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gCADIJteG816"
   },
   "outputs": [],
   "source": [
    "class MNISTFraction:\n",
    "    \"\"\"A helper class to extract only a fixed fraction of MNIST data.\"\"\"\n",
    "    def __init__(self, mnist, fraction):\n",
    "        self.mnist = mnist\n",
    "        self.num_images = int(mnist.num_examples * fraction)\n",
    "        self.image_data, self.label_data = mnist.images[:self.num_images], mnist.labels[:self.num_images]\n",
    "        self.start = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.start\n",
    "        end = min(start + batch_size, self.num_images)\n",
    "        self.start = 0 if end == self.num_images else end\n",
    "        \n",
    "        return self.image_data[start:end], self.label_data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "C52suFUDsami"
   },
   "outputs": [],
   "source": [
    "def train_tf_model(tf_model,                                     \n",
    "                   session,    # The active session.\n",
    "                   num_epochs,    # Max epochs/iterations to train for.\n",
    "                   batch_size=50,    # Number of examples per batch.\n",
    "                   keep_prob=1.0,    # (1. - dropout) probability, none by default.\n",
    "                   train_only_on_fraction=1.,    # Fraction of training data to use.\n",
    "                   optimizer_fn=None,    # The optimizer we want to use\n",
    "                   report_every=1, # Report training results every nr of epochs.\n",
    "                   eval_every=1,    # Evaluate on validation data every nr of epochs.\n",
    "                   stop_early=True,    # Use early stopping or not.\n",
    "                   verbose=True): \n",
    "\n",
    "    # Get the (symbolic) model input, output, loss and accuracy.\n",
    "    x, y = tf_model.x, tf_model.y\n",
    "    loss = tf_model.loss\n",
    "    accuracy = tf_model.accuracy()\n",
    "\n",
    "    # Compute the gradient of the loss with respect to the model parameters \n",
    "    # and create an op that will perform one parameter update using the specific\n",
    "    # optimizer's update rule in the direction of the gradients.\n",
    "    if optimizer_fn is None:\n",
    "        optimizer_fn = tf.train.AdamOptimizer()\n",
    "    optimizer_step = optimizer_fn.minimize(loss)\n",
    "\n",
    "    # Get the op which, when executed, will initialize the variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Actually initialize the variables (run the op).\n",
    "    session.run(init)\n",
    "\n",
    "    # Save the training loss and accuracies on training and validation data.\n",
    "    train_costs = []\n",
    "    train_accs = []\n",
    "    val_costs = []\n",
    "    val_accs = []\n",
    "\n",
    "    if train_only_on_fraction < 1:\n",
    "        mnist_train_data = MNISTFraction(mnist.train, train_only_on_fraction)\n",
    "    else:\n",
    "        mnist_train_data = mnist.train\n",
    "    \n",
    "    prev_c_eval = 1000000\n",
    "    \n",
    "    # Main training cycle.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batches = int(train_only_on_fraction * mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches.\n",
    "        for i in range(total_batches):\n",
    "            batch_x, batch_y = mnist_train_data.next_batch(batch_size)\n",
    "                        \n",
    "            # Run optimization op (backprop) and cost op (to get loss value),\n",
    "            # and compute the accuracy of the model.\n",
    "            feed_dict = {x: batch_x, y: batch_y}\n",
    "            if keep_prob < 1.:\n",
    "                feed_dict[\"keep_prob:0\"] = keep_prob\n",
    "                \n",
    "            _, c, a = session.run([optimizer_step, loss, accuracy], feed_dict=feed_dict)\n",
    "                        \n",
    "            # Compute average loss/accuracy\n",
    "            avg_cost += c / total_batches\n",
    "            avg_acc += a / total_batches            \n",
    "        \n",
    "        train_costs.append((epoch, avg_cost))\n",
    "        train_accs.append((epoch, avg_acc))\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % report_every == 0 and verbose:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Training cost=\", \"{:.9f}\".format(avg_cost))\n",
    "                \n",
    "        if epoch % eval_every == 0:\n",
    "            val_x, val_y = mnist.validation.images, mnist.validation.labels            \n",
    "            \n",
    "            feed_dict = {x : val_x, y : val_y}\n",
    "            if keep_prob < 1.:\n",
    "                feed_dict['keep_prob:0'] = 1.0\n",
    "                \n",
    "            c_eval, a_eval = session.run([loss, accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"Validation acc=\", \"{:.9f}\".format(a_eval))\n",
    "                \n",
    "            if c_eval >= prev_c_eval and stop_early:\n",
    "                print(\"Validation loss stopped improving, stopping training early after %d epochs!\" % (epoch + 1))\n",
    "                break\n",
    "                \n",
    "            prev_c_eval = c_eval\n",
    "                \n",
    "            val_costs.append((epoch, c_eval))\n",
    "            val_accs.append((epoch, a_eval))\n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    return train_costs, train_accs, val_costs, val_accs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AecVBHLe2USG"
   },
   "outputs": [],
   "source": [
    "# Helper functions to plot training progress.\n",
    "\n",
    "def my_plot(list_of_tuples):\n",
    "    \"\"\"Take a list of (epoch, value) and split these into lists of \n",
    "    epoch-only and value-only. Pass these to plot to make sure we\n",
    "    line up the values at the correct time-steps.\n",
    "    \"\"\"\n",
    "    plt.plot(*zip(*list_of_tuples))\n",
    "\n",
    "def plot_multi(values_lst, labels_lst, y_label, x_label='epoch'):\n",
    "    # Plot multiple curves.\n",
    "    assert len(values_lst) == len(labels_lst)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    for v in values_lst:\n",
    "        my_plot(v)\n",
    "    plt.legend(labels_lst, loc='upper left')\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyJG8AQPL_BH"
   },
   "source": [
    "## Wrapping everything together and verifying that it works (10min)\n",
    "\n",
    "Once we have a training function, it is usually a good idea to train on a small amount of your data first to verify that everything is indeed working. We can put all the pieces together to achieve this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 289,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1198,
     "status": "ok",
     "timestamp": 1503071394643,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "guiSbTOIMAUm",
    "outputId": "32e73f8e-81de-4a8c-d7bb-1bd2286f8050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 11.367639403\n",
      "Epoch: 0001 Validation acc= 0.140200004\n",
      "Epoch: 0002 Training cost= 10.970869580\n",
      "Epoch: 0003 Training cost= 10.625609350\n",
      "Epoch: 0003 Validation acc= 0.142600000\n",
      "Epoch: 0004 Training cost= 10.321249723\n",
      "Epoch: 0005 Training cost= 10.049566373\n",
      "Epoch: 0005 Validation acc= 0.146400005\n",
      "Epoch: 0006 Training cost= 9.804254710\n",
      "Epoch: 0007 Training cost= 9.580430967\n",
      "Epoch: 0007 Validation acc= 0.151999995\n",
      "Epoch: 0008 Training cost= 9.374331479\n",
      "Epoch: 0009 Training cost= 9.183043150\n",
      "Epoch: 0009 Validation acc= 0.156599998\n",
      "Epoch: 0010 Training cost= 9.004260596\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.159\n"
     ]
    }
   ],
   "source": [
    "##### BUILD MODEL #####\n",
    "tf.reset_default_graph()    # Clear the graph.\n",
    "model = DNNClassifier()     # Choose model hyperparameters.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    ##### TRAIN MODEL #####\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = train_tf_model(\n",
    "            model,\n",
    "            session=sess,\n",
    "            num_epochs=10, \n",
    "            train_only_on_fraction=1e-1,\n",
    "            optimizer_fn=tf.train.GradientDescentOptimizer(learning_rate=1e-3),\n",
    "            report_every=1,\n",
    "            eval_every=2,\n",
    "            stop_early=False) \n",
    "\n",
    "    ##### EVALUATE MODEL ON TEST DATA #####\n",
    "\n",
    "    # Get the op which calculates model accuracy.\n",
    "    accuracy_op = model.accuracy()    # Get the symbolic accuracy operation\n",
    "    \n",
    "    # Connect the MNIST test images and labels to the model input/output\n",
    "    # placeholders, and compute the accuracy given the trained parameters.\n",
    "    accuracy = accuracy_op.eval(feed_dict = {model.x: mnist.test.images, \n",
    "                                             model.y: mnist.test.labels})\n",
    "    \n",
    "    print(\"Accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCJs9hNt9LFV"
   },
   "source": [
    "Instead of just training and checking that the loss goes down, it is usually a good idea to try to **overfit a small subset of your training data**. We will do this below by training a 1 hidden layer network on a subset of the MNIST training data, by setting the `train_only_on_fraction` training hyperparameter to 0.05 (i.e. 5%). We turn off early stopping for this. The following diagram illustrates the difference between under-fitting and over-fitting. Note that the diagram is idealised and it's not always this clear in practice!\n",
    "\n",
    "![overfitting](images/over_and_under_fitting.png \"Over and Under Fitting\")\n",
    "\n",
    "**QUESTION**: Why do we turn off early-stopping?\n",
    "\n",
    "In the rest of this practical, we will explore the effects of different model hyperparameters and different training choices, so let's wrap everything together to emphasize these different choices, and then train a simple model for a few epochs on 5% of the data to verify that everything works and that we can overfit a small portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 3109,
     "output_extras": [
      {
       "item_id": 14
      },
      {
       "item_id": 25
      },
      {
       "item_id": 26
      },
      {
       "item_id": 27
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6426,
     "status": "ok",
     "timestamp": 1503068187309,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "SvA0W9wTuFbD",
    "outputId": "c03d1f7d-550a-48d1-d94a-e337b03e902f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 114.139924414\n",
      "Epoch: 0001 Validation acc= 0.457199991\n",
      "Epoch: 0002 Training cost= 39.938992899\n",
      "Epoch: 0003 Training cost= 24.148555253\n",
      "Epoch: 0003 Validation acc= 0.704800010\n",
      "Epoch: 0004 Training cost= 16.921453953\n",
      "Epoch: 0005 Training cost= 12.617178215\n",
      "Epoch: 0005 Validation acc= 0.767199993\n",
      "Epoch: 0006 Training cost= 9.536734265\n",
      "Epoch: 0007 Training cost= 7.447521800\n",
      "Epoch: 0007 Validation acc= 0.794200003\n",
      "Epoch: 0008 Training cost= 5.912098232\n",
      "Epoch: 0009 Training cost= 4.884312411\n",
      "Epoch: 0009 Validation acc= 0.804799974\n",
      "Epoch: 0010 Training cost= 3.931517785\n",
      "Epoch: 0011 Training cost= 3.226381225\n",
      "Epoch: 0011 Validation acc= 0.810000002\n",
      "Epoch: 0012 Training cost= 2.643380479\n",
      "Epoch: 0013 Training cost= 1.961918629\n",
      "Epoch: 0013 Validation acc= 0.815400004\n",
      "Epoch: 0014 Training cost= 1.582231742\n",
      "Epoch: 0015 Training cost= 1.267165074\n",
      "Epoch: 0015 Validation acc= 0.819400012\n",
      "Epoch: 0016 Training cost= 0.975266358\n",
      "Epoch: 0017 Training cost= 0.722313639\n",
      "Epoch: 0017 Validation acc= 0.818199992\n",
      "Epoch: 0018 Training cost= 0.566383661\n",
      "Epoch: 0019 Training cost= 0.426592244\n",
      "Epoch: 0019 Validation acc= 0.822799981\n",
      "Epoch: 0020 Training cost= 0.371580340\n",
      "Epoch: 0021 Training cost= 0.239731457\n",
      "Epoch: 0021 Validation acc= 0.824000001\n",
      "Epoch: 0022 Training cost= 0.203184221\n",
      "Epoch: 0023 Training cost= 0.160415682\n",
      "Epoch: 0023 Validation acc= 0.826200008\n",
      "Epoch: 0024 Training cost= 0.121091240\n",
      "Epoch: 0025 Training cost= 0.109282314\n",
      "Epoch: 0025 Validation acc= 0.826399982\n",
      "Epoch: 0026 Training cost= 0.085114442\n",
      "Epoch: 0027 Training cost= 0.069431303\n",
      "Epoch: 0027 Validation acc= 0.827600002\n",
      "Epoch: 0028 Training cost= 0.052500896\n",
      "Epoch: 0029 Training cost= 0.030858037\n",
      "Epoch: 0029 Validation acc= 0.829800010\n",
      "Epoch: 0030 Training cost= 0.026056554\n",
      "Epoch: 0031 Training cost= 0.009249896\n",
      "Epoch: 0031 Validation acc= 0.828000009\n",
      "Epoch: 0032 Training cost= 0.002624055\n",
      "Epoch: 0033 Training cost= 0.000305017\n",
      "Epoch: 0033 Validation acc= 0.832599998\n",
      "Epoch: 0034 Training cost= 0.000049026\n",
      "Epoch: 0035 Training cost= 0.000033517\n",
      "Epoch: 0035 Validation acc= 0.832599998\n",
      "Epoch: 0036 Training cost= 0.000026669\n",
      "Epoch: 0037 Training cost= 0.000022668\n",
      "Epoch: 0037 Validation acc= 0.832400024\n",
      "Epoch: 0038 Training cost= 0.000019932\n",
      "Epoch: 0039 Training cost= 0.000017908\n",
      "Epoch: 0039 Validation acc= 0.832199991\n",
      "Epoch: 0040 Training cost= 0.000016330\n",
      "Epoch: 0041 Training cost= 0.000015057\n",
      "Epoch: 0041 Validation acc= 0.832400024\n",
      "Epoch: 0042 Training cost= 0.000014003\n",
      "Epoch: 0043 Training cost= 0.000013111\n",
      "Epoch: 0043 Validation acc= 0.833000004\n",
      "Epoch: 0044 Training cost= 0.000012345\n",
      "Epoch: 0045 Training cost= 0.000011676\n",
      "Epoch: 0045 Validation acc= 0.833000004\n",
      "Epoch: 0046 Training cost= 0.000011087\n",
      "Epoch: 0047 Training cost= 0.000010563\n",
      "Epoch: 0047 Validation acc= 0.832799971\n",
      "Epoch: 0048 Training cost= 0.000010093\n",
      "Epoch: 0049 Training cost= 0.000009667\n",
      "Epoch: 0049 Validation acc= 0.833000004\n",
      "Epoch: 0050 Training cost= 0.000009280\n",
      "Epoch: 0051 Training cost= 0.000008925\n",
      "Epoch: 0051 Validation acc= 0.833000004\n",
      "Epoch: 0052 Training cost= 0.000008599\n",
      "Epoch: 0053 Training cost= 0.000008297\n",
      "Epoch: 0053 Validation acc= 0.832799971\n",
      "Epoch: 0054 Training cost= 0.000008017\n",
      "Epoch: 0055 Training cost= 0.000007756\n",
      "Epoch: 0055 Validation acc= 0.832799971\n",
      "Epoch: 0056 Training cost= 0.000007511\n",
      "Epoch: 0057 Training cost= 0.000007282\n",
      "Epoch: 0057 Validation acc= 0.832799971\n",
      "Epoch: 0058 Training cost= 0.000007066\n",
      "Epoch: 0059 Training cost= 0.000006862\n",
      "Epoch: 0059 Validation acc= 0.832599998\n",
      "Epoch: 0060 Training cost= 0.000006669\n",
      "Epoch: 0061 Training cost= 0.000006485\n",
      "Epoch: 0061 Validation acc= 0.832599998\n",
      "Epoch: 0062 Training cost= 0.000006312\n",
      "Epoch: 0063 Training cost= 0.000006145\n",
      "Epoch: 0063 Validation acc= 0.832599998\n",
      "Epoch: 0064 Training cost= 0.000005987\n",
      "Epoch: 0065 Training cost= 0.000005835\n",
      "Epoch: 0065 Validation acc= 0.832799971\n",
      "Epoch: 0066 Training cost= 0.000005689\n",
      "Epoch: 0067 Training cost= 0.000005550\n",
      "Epoch: 0067 Validation acc= 0.832799971\n",
      "Epoch: 0068 Training cost= 0.000005416\n",
      "Epoch: 0069 Training cost= 0.000005287\n",
      "Epoch: 0069 Validation acc= 0.832799971\n",
      "Epoch: 0070 Training cost= 0.000005163\n",
      "Epoch: 0071 Training cost= 0.000005042\n",
      "Epoch: 0071 Validation acc= 0.832599998\n",
      "Epoch: 0072 Training cost= 0.000004927\n",
      "Epoch: 0073 Training cost= 0.000004814\n",
      "Epoch: 0073 Validation acc= 0.832599998\n",
      "Epoch: 0074 Training cost= 0.000004705\n",
      "Epoch: 0075 Training cost= 0.000004600\n",
      "Epoch: 0075 Validation acc= 0.832599998\n",
      "Epoch: 0076 Training cost= 0.000004498\n",
      "Epoch: 0077 Training cost= 0.000004399\n",
      "Epoch: 0077 Validation acc= 0.832599998\n",
      "Epoch: 0078 Training cost= 0.000004303\n",
      "Epoch: 0079 Training cost= 0.000004209\n",
      "Epoch: 0079 Validation acc= 0.832599998\n",
      "Epoch: 0080 Training cost= 0.000004118\n",
      "Epoch: 0081 Training cost= 0.000004030\n",
      "Epoch: 0081 Validation acc= 0.832599998\n",
      "Epoch: 0082 Training cost= 0.000003943\n",
      "Epoch: 0083 Training cost= 0.000003860\n",
      "Epoch: 0083 Validation acc= 0.832599998\n",
      "Epoch: 0084 Training cost= 0.000003778\n",
      "Epoch: 0085 Training cost= 0.000003697\n",
      "Epoch: 0085 Validation acc= 0.832599998\n",
      "Epoch: 0086 Training cost= 0.000003620\n",
      "Epoch: 0087 Training cost= 0.000003544\n",
      "Epoch: 0087 Validation acc= 0.832799971\n",
      "Epoch: 0088 Training cost= 0.000003469\n",
      "Epoch: 0089 Training cost= 0.000003397\n",
      "Epoch: 0089 Validation acc= 0.832799971\n",
      "Epoch: 0090 Training cost= 0.000003326\n",
      "Epoch: 0091 Training cost= 0.000003257\n",
      "Epoch: 0091 Validation acc= 0.832799971\n",
      "Epoch: 0092 Training cost= 0.000003190\n",
      "Epoch: 0093 Training cost= 0.000003124\n",
      "Epoch: 0093 Validation acc= 0.832799971\n",
      "Epoch: 0094 Training cost= 0.000003059\n",
      "Epoch: 0095 Training cost= 0.000002996\n",
      "Epoch: 0095 Validation acc= 0.833000004\n",
      "Epoch: 0096 Training cost= 0.000002934\n",
      "Epoch: 0097 Training cost= 0.000002874\n",
      "Epoch: 0097 Validation acc= 0.833000004\n",
      "Epoch: 0098 Training cost= 0.000002815\n",
      "Epoch: 0099 Training cost= 0.000002757\n",
      "Epoch: 0099 Validation acc= 0.832799971\n",
      "Epoch: 0100 Training cost= 0.000002700\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.8277\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE THE CODE TEMPLATE BELOW WRAPS ALL OF THE ABOVE CODE, AND EXPOSES ONLY THE \n",
    "# DIFFERENT HYPERPARAMETER CHOICES. MAKE SURE YOU UNDERSTAND EXACTLY HOW\n",
    "# THE ABOVE CODE WORKS FIRST. TO SAVE SOME SPACE, WE WILL COPY AND MODIFY THE \n",
    "# CODE BELOW TO BUILD AND TRAIN DIFFERENT MODELS IN THE REST OF THIS PRACTICAL. \n",
    "# YOU CAN USE WHICHEVER VERSION YOU PREFER FOR YOUR OWN EXPERIMENTS.\n",
    "\n",
    "# Helper to wrap building, training, evaluating and plotting model accuracy.\n",
    "\n",
    "def build_train_eval_and_plot(build_params, train_params, verbose=True):\n",
    "    tf.reset_default_graph()\n",
    "    m = DNNClassifier(**build_params)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Train model on the MNIST dataset.\n",
    "     \n",
    "        train_losses, train_accs, val_losses, val_accs = train_tf_model(\n",
    "                m, \n",
    "                sess,\n",
    "                verbose=verbose,\n",
    "                **train_params) \n",
    "        \n",
    "        # Now evaluate it on the test set:\n",
    "    \n",
    "        accuracy_op = m.accuracy()    # Get the symbolic accuracy operation\n",
    "        # Calculate the accuracy using the test images and labels.\n",
    "        accuracy = accuracy_op.eval({m.x: mnist.test.images, \n",
    "                                                                 m.y: mnist.test.labels})    \n",
    "        \n",
    "        if verbose: \n",
    "            print(\"Accuracy on test set:\", accuracy)\n",
    "            # Plot losses and accuracies.\n",
    "            plot_multi([train_losses, val_losses], ['train', 'val'], 'loss', 'epoch')\n",
    "            plot_multi([train_accs, val_accs], ['train', 'val'], 'accuracy', 'epoch')\n",
    "            \n",
    "        \n",
    "        ret = {'train_losses': train_losses, 'train_accs' : train_accs,\n",
    "                     'val_losses' : val_losses, 'val_accs' : val_accs,\n",
    "                     'test_acc' : accuracy}\n",
    "        \n",
    "        return m, ret\n",
    "\n",
    "#################################CODE TEMPLATE##################################\n",
    "# Specify the model hyperparameters (NOTE: All the defaults can be omitted):\n",
    "model_params = {\n",
    "        #'input_size' : 784,    # There are 28x28 = 784 pixels in MNIST images\n",
    "        'hidden_sizes' : [512], # List of hidden layer dimensions, empty for linear model.\n",
    "        #'output_size' : 10,    # There are 10 possible digit classes\n",
    "        #'act_fn' : tf.nn.relu,    # The activation function to use in the hidden layers\n",
    "        'l2_lambda' : 0.            # Strength of L2 regularization.\n",
    "}\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {'num_epochs' : 100,     # Max epochs/iterations to train for.\n",
    "                        #'batch_size' : 100,    # Number of examples per batch, 100 default.\n",
    "                        #'keep_prob' : 1.0,    # (1. - dropout) probability, none by default.\n",
    "                        'train_only_on_fraction' : 5e-2,    # Fraction of training data to use, 1. for everything.\n",
    "                        'optimizer_fn' : None,    # Optimizer, None for Adam.\n",
    "                        'report_every' : 1, # Report training results every nr of epochs.\n",
    "                        'eval_every' : 2,     # Evaluate on validation data every nr of epochs.\n",
    "                        'stop_early' : False,    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")\n",
    "\n",
    "###############################END CODE TEMPLATE################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-YKo75RW1iO"
   },
   "source": [
    "Above we plot the training loss vs the validation loss and the training accuracy vs the validation accuracy on only 5% (`train_only_on_fraction=5e-3`) of the training data (so that it doesn't take too long, and also so that our model can overfit easier). We see that the loss is coming down and the accuracies are going up, as expected! By training on a small subset of the training data, we established that \n",
    "\n",
    "* the data that the model is being trained on is hopefully not corrupt (this can happen during preprocessing, loading, etc), \n",
    "* our loss and gradients are likely correct, \n",
    "* our optimizer seems to do the right thing, \n",
    "* and generally, that our code probably works! \n",
    "\n",
    "**NOTE**: Notice the point where training loss/accuracy continues to improve, but validation accuracy starts to plateau? That is the point where the model starts to **overfit** the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32oVqFziKYcv"
   },
   "source": [
    "# Architectural Choices (15min)\n",
    "\n",
    "## Depth\n",
    "\n",
    "### A Linear 784-10 Model\n",
    "\n",
    "Let's evaluate the simple linear model trained on the full dataset and then add layers to see what effect this will have on the accuracy.\n",
    "\n",
    "**NOTE**: If you're unsure what the \"784-10\" notation means, scroll up and re-read the section on \"Building a Feed-forward Neural Network\" where we explain that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 1528,
     "output_extras": [
      {
       "item_id": 27
      },
      {
       "item_id": 28
      },
      {
       "item_id": 29
      },
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24080,
     "status": "ok",
     "timestamp": 1502999087933,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "mChxOFJnL_kt",
    "outputId": "ab2184b1-64b2-4166-e638-9ef35a99c663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 3.979346356\n",
      "Epoch: 0001 Validation acc= 0.720000029\n",
      "Epoch: 0002 Training cost= 1.173970768\n",
      "Epoch: 0002 Validation acc= 0.819400012\n",
      "Epoch: 0003 Training cost= 0.831302601\n",
      "Epoch: 0003 Validation acc= 0.857200027\n",
      "Epoch: 0004 Training cost= 0.684144043\n",
      "Epoch: 0004 Validation acc= 0.870599985\n",
      "Epoch: 0005 Training cost= 0.600522155\n",
      "Epoch: 0005 Validation acc= 0.877399981\n",
      "Epoch: 0006 Training cost= 0.544605434\n",
      "Epoch: 0006 Validation acc= 0.886200011\n",
      "Epoch: 0007 Training cost= 0.503139624\n",
      "Epoch: 0007 Validation acc= 0.890999973\n",
      "Epoch: 0008 Training cost= 0.472414856\n",
      "Epoch: 0008 Validation acc= 0.896399975\n",
      "Epoch: 0009 Training cost= 0.447278972\n",
      "Epoch: 0009 Validation acc= 0.898800015\n",
      "Epoch: 0010 Training cost= 0.426831512\n",
      "Epoch: 0010 Validation acc= 0.900399983\n",
      "Epoch: 0011 Training cost= 0.409658925\n",
      "Epoch: 0011 Validation acc= 0.902800024\n",
      "Epoch: 0012 Training cost= 0.394903696\n",
      "Epoch: 0012 Validation acc= 0.903800011\n",
      "Epoch: 0013 Training cost= 0.382545183\n",
      "Epoch: 0013 Validation acc= 0.907199979\n",
      "Epoch: 0014 Training cost= 0.371564864\n",
      "Epoch: 0014 Validation acc= 0.906400025\n",
      "Epoch: 0015 Training cost= 0.360660462\n",
      "Epoch: 0015 Validation acc= 0.908599973\n",
      "Epoch: 0016 Training cost= 0.353035707\n",
      "Epoch: 0016 Validation acc= 0.909200013\n",
      "Epoch: 0017 Training cost= 0.344835858\n",
      "Epoch: 0017 Validation acc= 0.911400020\n",
      "Epoch: 0018 Training cost= 0.337569909\n",
      "Epoch: 0018 Validation acc= 0.912800014\n",
      "Epoch: 0019 Training cost= 0.331768654\n",
      "Epoch: 0019 Validation acc= 0.911199987\n",
      "Epoch: 0020 Training cost= 0.325731057\n",
      "Epoch: 0020 Validation acc= 0.910000026\n",
      "Epoch: 0021 Training cost= 0.320519279\n",
      "Epoch: 0021 Validation acc= 0.913600028\n",
      "Epoch: 0022 Training cost= 0.315147422\n",
      "Epoch: 0022 Validation acc= 0.915199995\n",
      "Epoch: 0023 Training cost= 0.311182667\n",
      "Epoch: 0023 Validation acc= 0.913999975\n",
      "Epoch: 0024 Training cost= 0.306721582\n",
      "Epoch: 0024 Validation acc= 0.914799988\n",
      "Epoch: 0025 Training cost= 0.302786230\n",
      "Epoch: 0025 Validation acc= 0.915600002\n",
      "Epoch: 0026 Training cost= 0.299410915\n",
      "Epoch: 0026 Validation acc= 0.916199982\n",
      "Epoch: 0027 Training cost= 0.296091753\n",
      "Epoch: 0027 Validation acc= 0.913999975\n",
      "Epoch: 0028 Training cost= 0.292688939\n",
      "Epoch: 0028 Validation acc= 0.916199982\n",
      "Epoch: 0029 Training cost= 0.289494832\n",
      "Epoch: 0029 Validation acc= 0.916000009\n",
      "Validation loss stopped improving, stopping training early after 29 epochs!\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.9163\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.6 s, sys: 6.22 s, total: 38.8 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the linear model on the full dataset.\n",
    "\n",
    "################################################################################\n",
    "# Specify the model hyperparameters.\n",
    "model_params = {'l2_lambda' : 0.}\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {'num_epochs' : 50,     # Max epochs/iterations to train for.\n",
    "                   'optimizer_fn' : None,            # Now we're using Adam.\n",
    "                   'report_every' : 1, # Report training results every nr of epochs.\n",
    "                   'eval_every' : 1,     # Evaluate on validation data every nr of epochs.\n",
    "                   'stop_early' : True    \n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnaNJvLpYp9F"
   },
   "source": [
    "~91% is quite a bad score on MNIST! Now let's build a deeper model, to improve on that score. \n",
    "\n",
    "### 1 hidden layer: 784-512-10 Architecture w/ L2\n",
    "\n",
    "Notice that we add a bit of L2-regularization to our model.\n",
    "\n",
    "**QUESTION**: What does that do? What is the effect of removing it? Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 2293,
     "output_extras": [
      {
       "item_id": 49
      },
      {
       "item_id": 50
      },
      {
       "item_id": 51
      },
      {
       "item_id": 52
      },
      {
       "item_id": 53
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64958,
     "status": "ok",
     "timestamp": 1503008137397,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "26XgusXy8OTs",
    "outputId": "6e4c8098-3ded-40ca-9281-ebbfc7862953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 176.821376273\n",
      "Epoch: 0001 Validation acc= 0.891799986\n",
      "Epoch: 0002 Training cost= 114.372428880\n",
      "Epoch: 0002 Validation acc= 0.920599997\n",
      "Epoch: 0003 Training cost= 86.876626823\n",
      "Epoch: 0003 Validation acc= 0.929600000\n",
      "Epoch: 0004 Training cost= 68.870978199\n",
      "Epoch: 0004 Validation acc= 0.935000002\n",
      "Epoch: 0005 Training cost= 55.311025092\n",
      "Epoch: 0005 Validation acc= 0.940400004\n",
      "Epoch: 0006 Training cost= 44.503921481\n",
      "Epoch: 0006 Validation acc= 0.946200013\n",
      "Epoch: 0007 Training cost= 35.763617710\n",
      "Epoch: 0007 Validation acc= 0.945999980\n",
      "Epoch: 0008 Training cost= 28.700154362\n",
      "Epoch: 0008 Validation acc= 0.950399995\n",
      "Epoch: 0009 Training cost= 23.008252097\n",
      "Epoch: 0009 Validation acc= 0.950600028\n",
      "Epoch: 0010 Training cost= 18.392711143\n",
      "Epoch: 0010 Validation acc= 0.943000019\n",
      "Epoch: 0011 Training cost= 14.682333702\n",
      "Epoch: 0011 Validation acc= 0.948199987\n",
      "Epoch: 0012 Training cost= 11.746643137\n",
      "Epoch: 0012 Validation acc= 0.952600002\n",
      "Epoch: 0013 Training cost= 9.370329402\n",
      "Epoch: 0013 Validation acc= 0.955200016\n",
      "Epoch: 0014 Training cost= 7.450025385\n",
      "Epoch: 0014 Validation acc= 0.951799989\n",
      "Epoch: 0015 Training cost= 5.916417954\n",
      "Epoch: 0015 Validation acc= 0.959599972\n",
      "Epoch: 0016 Training cost= 4.701051400\n",
      "Epoch: 0016 Validation acc= 0.955200016\n",
      "Epoch: 0017 Training cost= 3.714150565\n",
      "Epoch: 0017 Validation acc= 0.962400019\n",
      "Epoch: 0018 Training cost= 2.924387509\n",
      "Epoch: 0018 Validation acc= 0.953800023\n",
      "Epoch: 0019 Training cost= 2.290554577\n",
      "Epoch: 0019 Validation acc= 0.954400003\n",
      "Epoch: 0020 Training cost= 1.796939616\n",
      "Epoch: 0020 Validation acc= 0.956399977\n",
      "Epoch: 0021 Training cost= 1.408448726\n",
      "Epoch: 0021 Validation acc= 0.932200015\n",
      "Epoch: 0022 Training cost= 1.098742790\n",
      "Epoch: 0022 Validation acc= 0.955999970\n",
      "Epoch: 0023 Training cost= 0.855509135\n",
      "Epoch: 0023 Validation acc= 0.961199999\n",
      "Epoch: 0024 Training cost= 0.671426288\n",
      "Epoch: 0024 Validation acc= 0.966000021\n",
      "Epoch: 0025 Training cost= 0.536424664\n",
      "Epoch: 0025 Validation acc= 0.970600009\n",
      "Epoch: 0026 Training cost= 0.434293608\n",
      "Epoch: 0026 Validation acc= 0.972599983\n",
      "Epoch: 0027 Training cost= 0.360488848\n",
      "Epoch: 0027 Validation acc= 0.973399997\n",
      "Epoch: 0028 Training cost= 0.310232339\n",
      "Epoch: 0028 Validation acc= 0.970799983\n",
      "Epoch: 0029 Training cost= 0.272508446\n",
      "Epoch: 0029 Validation acc= 0.967599988\n",
      "Epoch: 0030 Training cost= 0.247107994\n",
      "Epoch: 0030 Validation acc= 0.976800025\n",
      "Epoch: 0031 Training cost= 0.227062772\n",
      "Epoch: 0031 Validation acc= 0.973800004\n",
      "Epoch: 0032 Training cost= 0.215026367\n",
      "Epoch: 0032 Validation acc= 0.975600004\n",
      "Epoch: 0033 Training cost= 0.206853231\n",
      "Epoch: 0033 Validation acc= 0.974799991\n",
      "Epoch: 0034 Training cost= 0.199584720\n",
      "Epoch: 0034 Validation acc= 0.972000003\n",
      "Epoch: 0035 Training cost= 0.196024069\n",
      "Epoch: 0035 Validation acc= 0.973200023\n",
      "Epoch: 0036 Training cost= 0.191828477\n",
      "Epoch: 0036 Validation acc= 0.974600017\n",
      "Epoch: 0037 Training cost= 0.187802641\n",
      "Epoch: 0037 Validation acc= 0.972199976\n",
      "Validation loss stopped improving, stopping training early after 37 epochs!\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.9712\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 45s, sys: 31 s, total: 7min 16s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Specify the model hyperparameters (NOTE: All the defaults can be omitted):\n",
    "model_params = {\n",
    "        'hidden_sizes' : [512],    # List of hidden layer dimensions, empty for linear model.\n",
    "        'l2_lambda' : 1e-3            # Strength of L2 regularization.\n",
    "}\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {\n",
    "        'num_epochs' : 50,        # Max epochs/iterations to train for.\n",
    "        'report_every' : 1,     # Report training results every nr of epochs.\n",
    "        'eval_every' : 1,         # Evaluate on validation data every nr of epochs.\n",
    "        'stop_early' : True    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhfzBJHw_ep6"
   },
   "source": [
    "**97.7%** is a much more decent score! The 1 hidden layer model gives a much better score than the linear model, so let's see if we can do better by adding another layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJ_Ec2df8YNj"
   },
   "source": [
    "### Going deeper! A 784-512-512-10 architecture w/ L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 4044,
     "output_extras": [
      {
       "item_id": 49
      },
      {
       "item_id": 98
      },
      {
       "item_id": 100
      },
      {
       "item_id": 101
      },
      {
       "item_id": 102
      },
      {
       "item_id": 103
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166883,
     "status": "ok",
     "timestamp": 1502969316725,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "3jALpxhzC6yc",
    "outputId": "874ec485-034b-44e5-a771-65bbd620a817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 482.650715942\n",
      "Epoch: 0001 Validation acc= 0.915400028\n",
      "Epoch: 0002 Training cost= 324.735924294\n",
      "Epoch: 0002 Validation acc= 0.939000010\n",
      "Epoch: 0003 Training cost= 289.963919178\n",
      "Epoch: 0003 Validation acc= 0.943000019\n",
      "Epoch: 0004 Training cost= 267.174931724\n",
      "Epoch: 0004 Validation acc= 0.953800023\n",
      "Epoch: 0005 Training cost= 248.735115093\n",
      "Epoch: 0005 Validation acc= 0.955399990\n",
      "Epoch: 0006 Training cost= 233.206841500\n",
      "Epoch: 0006 Validation acc= 0.960799992\n",
      "Epoch: 0007 Training cost= 219.021586470\n",
      "Epoch: 0007 Validation acc= 0.950999975\n",
      "Epoch: 0008 Training cost= 206.817723680\n",
      "Epoch: 0008 Validation acc= 0.959399998\n",
      "Epoch: 0009 Training cost= 195.015378168\n",
      "Epoch: 0009 Validation acc= 0.964999974\n",
      "Epoch: 0010 Training cost= 184.170884497\n",
      "Epoch: 0010 Validation acc= 0.959999979\n",
      "Epoch: 0011 Training cost= 173.968027385\n",
      "Epoch: 0011 Validation acc= 0.964200020\n",
      "Epoch: 0012 Training cost= 164.182535137\n",
      "Epoch: 0012 Validation acc= 0.966199994\n",
      "Epoch: 0013 Training cost= 155.036306083\n",
      "Epoch: 0013 Validation acc= 0.968599975\n",
      "Epoch: 0014 Training cost= 146.144501315\n",
      "Epoch: 0014 Validation acc= 0.966000021\n",
      "Epoch: 0015 Training cost= 138.071170002\n",
      "Epoch: 0015 Validation acc= 0.966600001\n",
      "Epoch: 0016 Training cost= 129.973220367\n",
      "Epoch: 0016 Validation acc= 0.970399976\n",
      "Epoch: 0017 Training cost= 122.452263655\n",
      "Epoch: 0017 Validation acc= 0.967199981\n",
      "Epoch: 0018 Training cost= 115.226178429\n",
      "Epoch: 0018 Validation acc= 0.969799995\n",
      "Epoch: 0019 Training cost= 108.395438323\n",
      "Epoch: 0019 Validation acc= 0.969600022\n",
      "Epoch: 0020 Training cost= 101.974610956\n",
      "Epoch: 0020 Validation acc= 0.966799974\n",
      "Epoch: 0021 Training cost= 95.603432361\n",
      "Epoch: 0021 Validation acc= 0.969200015\n",
      "Epoch: 0022 Training cost= 89.584447119\n",
      "Epoch: 0022 Validation acc= 0.969600022\n",
      "Epoch: 0023 Training cost= 83.935072431\n",
      "Epoch: 0023 Validation acc= 0.970399976\n",
      "Epoch: 0024 Training cost= 78.557491164\n",
      "Epoch: 0024 Validation acc= 0.974799991\n",
      "Epoch: 0025 Training cost= 73.370968330\n",
      "Epoch: 0025 Validation acc= 0.972599983\n",
      "Epoch: 0026 Training cost= 68.570242760\n",
      "Epoch: 0026 Validation acc= 0.974200010\n",
      "Epoch: 0027 Training cost= 63.883477325\n",
      "Epoch: 0027 Validation acc= 0.966000021\n",
      "Epoch: 0028 Training cost= 59.563897462\n",
      "Epoch: 0028 Validation acc= 0.970799983\n",
      "Epoch: 0029 Training cost= 55.292011972\n",
      "Epoch: 0029 Validation acc= 0.974799991\n",
      "Epoch: 0030 Training cost= 51.441526895\n",
      "Epoch: 0030 Validation acc= 0.974399984\n",
      "Epoch: 0031 Training cost= 47.700310794\n",
      "Epoch: 0031 Validation acc= 0.972599983\n",
      "Epoch: 0032 Training cost= 44.107330617\n",
      "Epoch: 0032 Validation acc= 0.969799995\n",
      "Epoch: 0033 Training cost= 40.616009029\n",
      "Epoch: 0033 Validation acc= 0.972000003\n",
      "Epoch: 0034 Training cost= 37.385411006\n",
      "Epoch: 0034 Validation acc= 0.970000029\n",
      "Epoch: 0035 Training cost= 34.393258005\n",
      "Epoch: 0035 Validation acc= 0.974399984\n",
      "Epoch: 0036 Training cost= 31.434517992\n",
      "Epoch: 0036 Validation acc= 0.967400014\n",
      "Epoch: 0037 Training cost= 28.760280080\n",
      "Epoch: 0037 Validation acc= 0.973599970\n",
      "Epoch: 0038 Training cost= 26.134726573\n",
      "Epoch: 0038 Validation acc= 0.972199976\n",
      "Epoch: 0039 Training cost= 23.795323252\n",
      "Epoch: 0039 Validation acc= 0.968400002\n",
      "Epoch: 0040 Training cost= 21.527531038\n",
      "Epoch: 0040 Validation acc= 0.966400027\n",
      "Epoch: 0041 Training cost= 19.374497403\n",
      "Epoch: 0041 Validation acc= 0.972400010\n",
      "Epoch: 0042 Training cost= 17.442077248\n",
      "Epoch: 0042 Validation acc= 0.971199989\n",
      "Epoch: 0043 Training cost= 15.593516213\n",
      "Epoch: 0043 Validation acc= 0.971400023\n",
      "Epoch: 0044 Training cost= 13.879580294\n",
      "Epoch: 0044 Validation acc= 0.971400023\n",
      "Epoch: 0045 Training cost= 12.256050348\n",
      "Epoch: 0045 Validation acc= 0.971400023\n",
      "Epoch: 0046 Training cost= 10.843885443\n",
      "Epoch: 0046 Validation acc= 0.972800016\n",
      "Epoch: 0047 Training cost= 9.445476715\n",
      "Epoch: 0047 Validation acc= 0.964999974\n",
      "Epoch: 0048 Training cost= 8.238638694\n",
      "Epoch: 0048 Validation acc= 0.970200002\n",
      "Epoch: 0049 Training cost= 7.118942408\n",
      "Epoch: 0049 Validation acc= 0.969200015\n",
      "Epoch: 0050 Training cost= 6.098379751\n",
      "Epoch: 0050 Validation acc= 0.966199994\n",
      "Epoch: 0051 Training cost= 5.180798251\n",
      "Epoch: 0051 Validation acc= 0.966400027\n",
      "Epoch: 0052 Training cost= 4.346853700\n",
      "Epoch: 0052 Validation acc= 0.972400010\n",
      "Epoch: 0053 Training cost= 3.596992811\n",
      "Epoch: 0053 Validation acc= 0.972800016\n",
      "Epoch: 0054 Training cost= 2.939764108\n",
      "Epoch: 0054 Validation acc= 0.958400011\n",
      "Epoch: 0055 Training cost= 2.366096363\n",
      "Epoch: 0055 Validation acc= 0.964800000\n",
      "Epoch: 0056 Training cost= 1.839644543\n",
      "Epoch: 0056 Validation acc= 0.967599988\n",
      "Epoch: 0057 Training cost= 1.399133264\n",
      "Epoch: 0057 Validation acc= 0.970600009\n",
      "Epoch: 0058 Training cost= 1.041027024\n",
      "Epoch: 0058 Validation acc= 0.969799995\n",
      "Epoch: 0059 Training cost= 0.749825787\n",
      "Epoch: 0059 Validation acc= 0.974799991\n",
      "Epoch: 0060 Training cost= 0.553501541\n",
      "Epoch: 0060 Validation acc= 0.965600014\n",
      "Epoch: 0061 Training cost= 0.415815158\n",
      "Epoch: 0061 Validation acc= 0.971400023\n",
      "Epoch: 0062 Training cost= 0.326613824\n",
      "Epoch: 0062 Validation acc= 0.975799978\n",
      "Epoch: 0063 Training cost= 0.271339337\n",
      "Epoch: 0063 Validation acc= 0.977199972\n",
      "Epoch: 0064 Training cost= 0.236574774\n",
      "Epoch: 0064 Validation acc= 0.976199985\n",
      "Epoch: 0065 Training cost= 0.213750473\n",
      "Epoch: 0065 Validation acc= 0.975600004\n",
      "Epoch: 0066 Training cost= 0.199268678\n",
      "Epoch: 0066 Validation acc= 0.975000024\n",
      "Epoch: 0067 Training cost= 0.187568884\n",
      "Epoch: 0067 Validation acc= 0.976999998\n",
      "Epoch: 0068 Training cost= 0.180584211\n",
      "Epoch: 0068 Validation acc= 0.978799999\n",
      "Epoch: 0069 Training cost= 0.176753673\n",
      "Epoch: 0069 Validation acc= 0.978799999\n",
      "Epoch: 0070 Training cost= 0.170226842\n",
      "Epoch: 0070 Validation acc= 0.976000011\n",
      "Validation loss stopped improving, stopping training early after 70 epochs!\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.9778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 4s, sys: 1min 33s, total: 28min 37s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Specify the model hyperparameters (NOTE: All the defaults can be omitted):\n",
    "model_params = {\n",
    "        'hidden_sizes' : [512, 512], # List of hidden layer dimensions, empty for linear model.\n",
    "        'l2_lambda' : 1e-3                     # Strength of L2 regularization.\n",
    "}\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {\n",
    "        'num_epochs' : 200,     # Max epochs/iterations to train for.\n",
    "        'report_every' : 1,     # Report training results every nr of epochs.\n",
    "        'eval_every' : 1,         # Evaluate on validation data every nr of epochs.\n",
    "        'stop_early' : True,    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4F5tXNkiHSOQ"
   },
   "source": [
    "You should get around **97.4**%. Shouldn't deeper do better?! Why is it that the 2-hidden layer model\n",
    "\n",
    "* ***took much longer to train*** (2min 46s versus 1min 3s on our system), and\n",
    "* ***got roughly the same accuracy*** as the 1 hidden layer model (sometimes worse)?\n",
    "\n",
    "This illustrates the **fundamental difficulty of training deep networks** that have plagued deep learning research for decades (and to some extent, still do): \n",
    "\n",
    "> ***Although deeper networks can give you more powerful models, training those models to find the right parameters is not always easy & takes a lot of computing power (time)!***\n",
    "\n",
    "For a long time people just believed it wouldn't work because a) fewer people tried to make it work, and those who did b) didn't have enough data or computing power to really explore the vast space of hyperparameters. These days, we have more data and more compute power, however one can never have 'enough' resources :) This is where the art of doing proper hyper-parameter selection comes in. In fact, we should be able to squeeze out another percentage point or so by choosing better:\n",
    "\n",
    "* Optimizer + its hyperparams (learning rate, momentum rates, etc)\n",
    "* batch-size,\n",
    "* choice of regularization.\n",
    "\n",
    "For a new problem, we will typically spend most of our time exploring these choices, usually just by launching many different training runs (hopefully in parallel and using GPUs!) and keeping the best ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzyUXqInbqaB"
   },
   "source": [
    "# EXTRA: How to Choose Architectures?\n",
    "\n",
    "How does one design new architectures? Should you use:\n",
    "\n",
    "* a tapered architecture (large-medium-small),\n",
    "* a \"regular\" (like the Levi's :) architecture (large-med-med-...-small),\n",
    "* a \"bottlenecked\" architecutre (large-small-large)?\n",
    "* an \"over-complete\" architecture (small-large-small). \n",
    "\n",
    "Unfortunately it mostly comes down to just developing your own intuition over time, and trying different approaches in hyperparameter search. \n",
    "\n",
    "However, a good pattern to follow in general is something like the following:\n",
    "\n",
    "1. Start with a basic architecture and overfit a portion of your training data.\n",
    "2. As you train on more data, add capacity and prevent overfitting by\n",
    "    * Adding more units to the hidden layer (often times wide still beats deep)\n",
    "    * Add one or two more hidden layers, trying some architectural choices mentioned above.\n",
    "    * Add dropout.\n",
    "    \n",
    "The goal is to match your architecture to your data, meaning you have just enough capacity to fit the data, but not too much to easily overfit. The easiest way to do this is to gradually build up the capacity of your model in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkrPMrQoSfK0"
   },
   "source": [
    "# EXTRA: Hyperparameter Selection (USE A GPU INSTANCE FOR THIS)\n",
    "\n",
    "## Random grid-search\n",
    "\n",
    "Most of the deep learning practitioner's time will be spent training and evaluating new model architectures with different hyperparameter combinations.    General rules of thumb exist, but often times change for each new architecture or dataset. Several approaches exist for automatic hyperparameter selection (grid search, random search, Bayesian methods). In practice, most people just use a **randomized grid-search** approach to try out different hyperparams, where one defines sensible values for each hyperparameter, and then randomly samples from the (cross-product space) of all of these possible combinations. This space grows exponentially, making exhaustive search infeasible for all but the simplest models. In practice, randomized search have also been found to find the best values, quicker (see [Bergstra and Bengio 2009](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) if you're interested in the details).\n",
    "\n",
    "Below, we will illustrate the basic idea with a skeleton randomized grid search implementation. In practice, one would launch these different runs in parallel on a computing cluster, making it easier to explore multiple options at the same time. Here, we would just try out a few different options to get a sense for how that would work. \n",
    "\n",
    "**NOTE**: We will keep the models small here (so we won't get SOTA results), in order to keep the training times reasonable, but to illustrate how dependent results are on these choices.\n",
    "\n",
    "**Architecture Selection**: Let's consider 1 and 2-hidden layer models. For each layer, we'll try out [128, 256] neurons per layer. We'll stick to ReLUs for this.\n",
    "\n",
    "**Hyperparameters**: Let's pick SGD with Momentum as our optimizer (a good, stable workhorse), with a fixed computational budget of 20 training epochs. We'll need to pick ranges for the following:\n",
    "* *learning rate*: we'll use a log-scale from 1e-5 to 1.\n",
    "* *momentum coefficient*: we'll use a log-scale from 5e-1 (0.5) to 1.\n",
    "* *L2 regularization*: we'll use a log-scale from 1e-5 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 71,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1503658401647,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "kyf_7dWqd_p1",
    "outputId": "cb4464de-cff7-4eee-a798-e11eaef911ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_sizes': [128, 128], 'l2_lambda': 0.006400327800616545}\n",
      "{'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x124cea550>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n"
     ]
    }
   ],
   "source": [
    "def sample_log_scale(v_min=1e-6, v_max=1.):\n",
    "    '''Sample uniformly on a log-scale from 10**v_min to 10**v_max.'''\n",
    "    return np.exp(np.random.uniform(np.log(v_min), np.log(v_max)))\n",
    "\n",
    "\n",
    "def sample_model_architecture_and_hyperparams(max_num_layers=2,\n",
    "                                              lr_min=1e-6,\n",
    "                                              lr_max=1.,\n",
    "                                              mom_min=0.5,\n",
    "                                              mom_max=1.,\n",
    "                                              l2_min=1e-4,\n",
    "                                              l2_max=1.):\n",
    "        '''Generate a random model architecture & hyperparameters.'''\n",
    "        \n",
    "        # Sample the architecture.\n",
    "        num_layers = np.random.choice(range(1, max_num_layers+1))\n",
    "        hidden_sizes = []\n",
    "        layer_ranges=[128, 256]\n",
    "        for l in range(num_layers):\n",
    "            hidden_sizes.append(np.random.choice(layer_ranges))\n",
    "        \n",
    "        # Sample the training parameters.\n",
    "        l2_lambda = sample_log_scale(l2_min, l2_max)\n",
    "        lr = sample_log_scale(lr_min, lr_max)\n",
    "        mom_coeff = sample_log_scale(mom_min, mom_max)\n",
    "        \n",
    "        # Build base model definitions:\n",
    "        model_params = {\n",
    "                'hidden_sizes' : hidden_sizes,\n",
    "                'l2_lambda' : l2_lambda}\n",
    "\n",
    "        # Specify the training hyperparameters:\n",
    "        training_params = {\n",
    "                'num_epochs' : 20,\n",
    "                'optimizer_fn' : tf.train.MomentumOptimizer(\n",
    "                        learning_rate=lr, \n",
    "                        momentum=mom_coeff),\n",
    "                'report_every' : 1,\n",
    "                'eval_every' : 1,\n",
    "                'stop_early' : True}\n",
    "        \n",
    "        return model_params, training_params\n",
    "\n",
    "# TEST THIS: Run this cell a few times and look at the different outputs. \n",
    "# Each of these will be a different model trained with different hyperparameters.\n",
    "m, t = sample_model_architecture_and_hyperparams()\n",
    "print(m)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPXaIIZteNU-"
   },
   "source": [
    "We will play around with the strength of the L2 regularization. Let's use SGD+Momentum, and run for a fixed budget of 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 1108,
     "output_extras": [
      {
       "item_id": 10
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 189573,
     "status": "ok",
     "timestamp": 1503227140737,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "Kyb7EOJWiehX",
    "outputId": "f221c2a9-4443-4a6d-977c-5b65e5087b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: 0 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [256], 'l2_lambda': 0.000285562584266694}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2198>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 1 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [256, 256], 'l2_lambda': 0.006766549378200129}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x1231bbe48>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 2 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [256], 'l2_lambda': 0.009249847342616226}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2b38>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Validation loss stopped improving, stopping training early after 4 epochs!\n",
      "Optimization Finished!\n",
      "RUN: 3 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [128, 128], 'l2_lambda': 0.0024143487465526567}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2668>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 4 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [128], 'l2_lambda': 0.004737378064547783}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118cefa90>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Validation loss stopped improving, stopping training early after 7 epochs!\n",
      "Optimization Finished!\n",
      "RUN: 5 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [128, 256], 'l2_lambda': 0.060071240197161806}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x122e8fda0>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Validation loss stopped improving, stopping training early after 2 epochs!\n",
      "Optimization Finished!\n",
      "RUN: 6 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [128], 'l2_lambda': 0.0011738587908220323}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118debf28>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 7 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [128, 256], 'l2_lambda': 0.044973777318023156}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x11abf7dd8>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 8 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [256], 'l2_lambda': 0.00043738954899184775}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x119111128>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n",
      "RUN: 9 out of 10:\n",
      "Sampled Architecture: \n",
      " {'hidden_sizes': [256], 'l2_lambda': 0.20865186302342192}\n",
      "Hyper-parameters:\n",
      " {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x11af04d68>, 'report_every': 1, 'eval_every': 1, 'stop_early': True}\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Perform a random search over hyper-parameter space this many times.\n",
    "NUM_EXPERIMENTS = 10\n",
    "\n",
    "for i in range(NUM_EXPERIMENTS): \n",
    "    \n",
    "    # Sample the model and hyperparams we are using.\n",
    "    model_params, training_params = sample_model_architecture_and_hyperparams()\n",
    "    \n",
    "    print(\"RUN: %d out of %d:\" % (i, NUM_EXPERIMENTS))\n",
    "    print(\"Sampled Architecture: \\n\", model_params)\n",
    "    print(\"Hyper-parameters:\\n\", training_params)\n",
    "    \n",
    "    # Build, train, evaluate\n",
    "    model, performance = build_train_eval_and_plot(\n",
    "            model_params, training_params, verbose=False)\n",
    "    \n",
    "    # Save results\n",
    "    results.append((performance['test_acc'], model_params, training_params))\n",
    "    \n",
    "# Display (best?) results/variance/etc:\n",
    "results.sort(key=lambda x : x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 207,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1503228968350,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "A6E0f3M02xB9",
    "outputId": "87d3d8a3-36dc-4f5a-d720-4ef6762f986a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.96, {'hidden_sizes': [256, 256], 'l2_lambda': 0.006766549378200129}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x1231bbe48>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.9569, {'hidden_sizes': [128], 'l2_lambda': 0.004737378064547783}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118cefa90>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.9312, {'hidden_sizes': [256], 'l2_lambda': 0.000285562584266694}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2198>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.923, {'hidden_sizes': [256], 'l2_lambda': 0.009249847342616226}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2b38>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.8683, {'hidden_sizes': [128], 'l2_lambda': 0.0011738587908220323}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118debf28>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.7629, {'hidden_sizes': [128, 128], 'l2_lambda': 0.0024143487465526567}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x118ad2668>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.6765, {'hidden_sizes': [256], 'l2_lambda': 0.00043738954899184775}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x119111128>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.4226, {'hidden_sizes': [128, 256], 'l2_lambda': 0.044973777318023156}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x11abf7dd8>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.098, {'hidden_sizes': [256], 'l2_lambda': 0.20865186302342192}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x11af04d68>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n",
      "(0.0892, {'hidden_sizes': [128, 256], 'l2_lambda': 0.060071240197161806}, {'num_epochs': 20, 'optimizer_fn': <tensorflow.python.training.momentum.MomentumOptimizer object at 0x122e8fda0>, 'report_every': 1, 'eval_every': 1, 'stop_early': True})\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r)    # Tuples of (test_accuracy, model_hyperparameters, training_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgu2OxMS41sF"
   },
   "source": [
    "**Notice the huuuuge variance in the test accuracies!** This is just a toy run, but hopefully it illustrates how important choosing the right architectures and training hyperparameters is to getting good results in deep learning.\n",
    "\n",
    "Below, we've included some hyperparameter settings which achieve near state-of-the-art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsCJW0_gRFX8"
   },
   "source": [
    "# EXTRA: Known Good Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0pO_4fQm8xi"
   },
   "source": [
    "## 784-500-300-10 w/ L2 + SGD + Momentum\n",
    "\n",
    "**Best so far: 98.02% test accuracy when we ran this (well, in one of our training runs :).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 3602,
     "output_extras": [
      {
       "item_id": 49
      },
      {
       "item_id": 88
      },
      {
       "item_id": 89
      },
      {
       "item_id": 90
      },
      {
       "item_id": 91
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 129328,
     "status": "ok",
     "timestamp": 1503057210512,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "J1spLw65l7mY",
    "outputId": "ac425ebc-96c1-4025-e493-ad3818bfc5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 299.901663624\n",
      "Epoch: 0001 Validation acc= 0.184000000\n",
      "Epoch: 0002 Training cost= 218.648436834\n",
      "Epoch: 0002 Validation acc= 0.189799994\n",
      "Epoch: 0003 Training cost= 175.570795288\n",
      "Epoch: 0003 Validation acc= 0.339800000\n",
      "Epoch: 0004 Training cost= 140.954676854\n",
      "Epoch: 0004 Validation acc= 0.347799987\n",
      "Epoch: 0005 Training cost= 113.180692430\n",
      "Epoch: 0005 Validation acc= 0.446000010\n",
      "Epoch: 0006 Training cost= 90.909929643\n",
      "Epoch: 0006 Validation acc= 0.504599988\n",
      "Epoch: 0007 Training cost= 72.807562984\n",
      "Epoch: 0007 Validation acc= 0.660600007\n",
      "Epoch: 0008 Training cost= 58.235078888\n",
      "Epoch: 0008 Validation acc= 0.772800028\n",
      "Epoch: 0009 Training cost= 46.594354449\n",
      "Epoch: 0009 Validation acc= 0.871200025\n",
      "Epoch: 0010 Training cost= 37.300518043\n",
      "Epoch: 0010 Validation acc= 0.920599997\n",
      "Epoch: 0011 Training cost= 29.883832814\n",
      "Epoch: 0011 Validation acc= 0.931200027\n",
      "Epoch: 0012 Training cost= 23.942953207\n",
      "Epoch: 0012 Validation acc= 0.940599978\n",
      "Epoch: 0013 Training cost= 19.201034457\n",
      "Epoch: 0013 Validation acc= 0.949999988\n",
      "Epoch: 0014 Training cost= 15.408415777\n",
      "Epoch: 0014 Validation acc= 0.951799989\n",
      "Epoch: 0015 Training cost= 12.369316832\n",
      "Epoch: 0015 Validation acc= 0.958400011\n",
      "Epoch: 0016 Training cost= 9.934452609\n",
      "Epoch: 0016 Validation acc= 0.966199994\n",
      "Epoch: 0017 Training cost= 7.979145482\n",
      "Epoch: 0017 Validation acc= 0.968999982\n",
      "Epoch: 0018 Training cost= 6.417269793\n",
      "Epoch: 0018 Validation acc= 0.969600022\n",
      "Epoch: 0019 Training cost= 5.168246206\n",
      "Epoch: 0019 Validation acc= 0.972199976\n",
      "Epoch: 0020 Training cost= 4.163984934\n",
      "Epoch: 0020 Validation acc= 0.975399971\n",
      "Epoch: 0021 Training cost= 3.361043340\n",
      "Epoch: 0021 Validation acc= 0.972400010\n",
      "Epoch: 0022 Training cost= 2.719848701\n",
      "Epoch: 0022 Validation acc= 0.975000024\n",
      "Epoch: 0023 Training cost= 2.206787167\n",
      "Epoch: 0023 Validation acc= 0.975799978\n",
      "Epoch: 0024 Training cost= 1.793793074\n",
      "Epoch: 0024 Validation acc= 0.976800025\n",
      "Epoch: 0025 Training cost= 1.463946249\n",
      "Epoch: 0025 Validation acc= 0.978200018\n",
      "Epoch: 0026 Training cost= 1.200036681\n",
      "Epoch: 0026 Validation acc= 0.978399992\n",
      "Epoch: 0027 Training cost= 0.988469249\n",
      "Epoch: 0027 Validation acc= 0.976400018\n",
      "Epoch: 0028 Training cost= 0.818273368\n",
      "Epoch: 0028 Validation acc= 0.977599978\n",
      "Epoch: 0029 Training cost= 0.683206030\n",
      "Epoch: 0029 Validation acc= 0.977400005\n",
      "Epoch: 0030 Training cost= 0.574180315\n",
      "Epoch: 0030 Validation acc= 0.977199972\n",
      "Epoch: 0031 Training cost= 0.485822938\n",
      "Epoch: 0031 Validation acc= 0.980000019\n",
      "Epoch: 0032 Training cost= 0.417628629\n",
      "Epoch: 0032 Validation acc= 0.979399979\n",
      "Epoch: 0033 Training cost= 0.360801130\n",
      "Epoch: 0033 Validation acc= 0.980400026\n",
      "Epoch: 0034 Training cost= 0.315974011\n",
      "Epoch: 0034 Validation acc= 0.978200018\n",
      "Epoch: 0035 Training cost= 0.279612325\n",
      "Epoch: 0035 Validation acc= 0.978399992\n",
      "Epoch: 0036 Training cost= 0.250157870\n",
      "Epoch: 0036 Validation acc= 0.981199980\n",
      "Epoch: 0037 Training cost= 0.226939937\n",
      "Epoch: 0037 Validation acc= 0.978999972\n",
      "Epoch: 0038 Training cost= 0.206719878\n",
      "Epoch: 0038 Validation acc= 0.981599987\n",
      "Epoch: 0039 Training cost= 0.192013404\n",
      "Epoch: 0039 Validation acc= 0.980000019\n",
      "Epoch: 0040 Training cost= 0.179165129\n",
      "Epoch: 0040 Validation acc= 0.981000006\n",
      "Epoch: 0041 Training cost= 0.168972922\n",
      "Epoch: 0041 Validation acc= 0.978600025\n",
      "Epoch: 0042 Training cost= 0.161435916\n",
      "Epoch: 0042 Validation acc= 0.980199993\n",
      "Epoch: 0043 Training cost= 0.155524970\n",
      "Epoch: 0043 Validation acc= 0.981400013\n",
      "Epoch: 0044 Training cost= 0.149086107\n",
      "Epoch: 0044 Validation acc= 0.981800020\n",
      "Epoch: 0045 Training cost= 0.144102089\n",
      "Epoch: 0045 Validation acc= 0.980400026\n",
      "Epoch: 0046 Training cost= 0.141289218\n",
      "Epoch: 0046 Validation acc= 0.980599999\n",
      "Epoch: 0047 Training cost= 0.136392120\n",
      "Epoch: 0047 Validation acc= 0.979399979\n",
      "Epoch: 0048 Training cost= 0.135019135\n",
      "Epoch: 0048 Validation acc= 0.979799986\n",
      "Epoch: 0049 Training cost= 0.132552108\n",
      "Epoch: 0049 Validation acc= 0.980599999\n",
      "Epoch: 0050 Training cost= 0.130667001\n",
      "Epoch: 0050 Validation acc= 0.981199980\n",
      "Epoch: 0051 Training cost= 0.129654578\n",
      "Epoch: 0051 Validation acc= 0.981000006\n",
      "Validation loss stopped improving, stopping training early after 51 epochs!\n",
      "Optimization Finished!\n",
      "Accuracy on test set: 0.9788\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 28s, sys: 57.4 s, total: 13min 25s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Specify the model hyperparameters (NOTE: All the defaults can be omitted):\n",
    "model_params = {\n",
    "        'hidden_sizes' : [500, 300], # List of hidden layer dimensions, empty for linear model.\n",
    "        'l2_lambda' : 1e-3                     # Strength of L2 regularization.\n",
    "}\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {'num_epochs' : 100,     # Max epochs/iterations to train for.\n",
    "                        'optimizer_fn' : tf.train.MomentumOptimizer(learning_rate=2e-3, momentum=0.98),\n",
    "                        'report_every' : 1, # Report training results every nr of epochs.\n",
    "                        'eval_every' : 1,     # Evaluate on validation data every nr of epochs.\n",
    "                        'stop_early' : True,    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tB5r6-oLTzgE"
   },
   "source": [
    "# NB: Before you go (5min)\n",
    "\n",
    "Pair up with someone else and go through the questions in \"Learning Objectives\" at the top. Take turns explaining each of these to each other, and be sure to ask the tutors if you're both unsure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ryLRtd2XfeX"
   },
   "source": [
    "# Additional Resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTnXEgI1oMY9"
   },
   "source": [
    "* TensorFlow Debugging (useful tips and code patterns): https://github.com/wookayin/tensorflow-talk-debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZrPt_Ao3z1U"
   },
   "source": [
    "# Feedback\n",
    "\n",
    "Please send any bugs and comments to dli-practicals@googlegroups.com."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Practical 2: Deep MNIST and Best Practices (Solution)",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "venv_deep_learning_p3",
   "language": "python",
   "name": "venv_deep_learning_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
